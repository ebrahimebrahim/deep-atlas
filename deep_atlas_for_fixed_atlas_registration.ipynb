{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2297e7a8",
   "metadata": {},
   "source": [
    "WIP.\n",
    "\n",
    "This will be a modification of the original DeepAtlas tutorial, intended for creating a registration model for registering images to a given fixed atlas image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697b614",
   "metadata": {},
   "source": [
    "# DeepAtlas\n",
    "\n",
    "This tutorial demonstrates the use of MONAI for simultaneous training of a registration and a segmentation network, following the approach of [DeepAtlas](https://biag.cs.unc.edu/publication/dblp-confmiccai-xu-n-19/).\n",
    "\n",
    "> Xu Z., Niethammer M. (2019) DeepAtlas: Joint Semi-supervised Learning of Image Registration and Segmentation. In: Shen D. et al. (eds) Medical Image Computing and Computer Assisted Intervention â€“ MICCAI 2019. MICCAI 2019. Lecture Notes in Computer Science, vol 11765. Springer, Cham. https://doi.org/10.1007/978-3-030-32245-8_47\n",
    "\n",
    "![deep atlas framework diagram](notebook_resources/paper_fig.png)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook works with 3D images and segmentations, and it results in two trained models: a segmentation network `seg_net` and a registration network `reg_net`. There are two training phases: a pre-training phase for the segmentation network alone, and an alternating training phase for the two networks together. Use of the trained models for inference is demonstrated at the end of the notebook.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/master/deep_atlas/deep_atlas_tutorial.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Contents:**\n",
    "* [Introduction](#Introduction)\n",
    "* [Setup Environment and Imports](#Setup-Environment-and-Imports)\n",
    "* [Load and Transform Data](#Load-and-Transform-Data)\n",
    "    * [Get data](#Get-data)\n",
    "    * [Resize and device options](#Resize-and-device-options)\n",
    "    * [Get SRI24 atlas](#Get-SRI24-atlas)\n",
    "    * [Create a list of data items](#Create-a-list-of-data-items)\n",
    "    * [Datasets for segmentation network pre-training](#Datasets-for-segmentation-network-pre-training)\n",
    "    * [Datasets for training both registration and segmentation networks](#Datasets-for-training-both-registration-and-segmentation-networks)\n",
    "    * [Load SRI24 atlas and probability maps](#Load-SRI24-atlas-and-probability-maps)\n",
    "* [Create Segmentation Network](#Create-Segmentation-Network)\n",
    "    * [Dice loss](#Dice-loss)\n",
    "* [Create Registration Network](#Create-Registration-Network)\n",
    "    * [Image warping](#Image-warping)\n",
    "    * [Image similarity loss](#Image-similarity-loss)\n",
    "    * [Regularization loss](#Regularization-loss)\n",
    "* [Train Networks](#Train-Networks)\n",
    "    * [Pre-train segmentation network](#Pre-train-segmentation-network)\n",
    "    * [Train both networks in alternation](#Train-both-networks-in-alternation)\n",
    "        * [Prepare components to use in the training loop](#Prepare-components-to-use-in-the-training-loop)\n",
    "        * [Training loop](#Training-loop)\n",
    "* [Visualize results](#Visualize-results)\n",
    "    * [Inference using segmentation network](#Inference-using-segmentation-network)\n",
    "    * [Inference using registration network](#Inference-using-registration-network)\n",
    "    * [Count folds](#Count-folds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae94f11",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Manual segmentation is expensive in the world of medical image data.\n",
    "The task requires experts, and additionally the images are often three dimensional,\n",
    "making segmentation a lot more laborious.\n",
    "Consequently, we often find ourselves with an abundance of images and a shortage of segmentation labels.\n",
    "The DeepAtlas framework was designed for this situation.\n",
    "\n",
    "The guiding concept is that registration models and segmentation models can support each other:\n",
    "- _Seg helps reg:_ Suppose we have a great segmentation algorithm that can label the anatomical features in some class of images. This can be used to evaluate the performance of a registration model:\n",
    "  1. Register an image pair, deforming a \"moving\" image to a \"target\" image.\n",
    "  2. Obtain segmentations for the two images.\n",
    "  3. Use the deformation from the registration to warp the moving image's segmentation, obtaining a candidate segmentation for the target image.\n",
    "  4. If the warped moving image segmentation matches the target image segmentation, then the registration algorithm performed well.\n",
    "  \n",
    "  If we want a deep neural network to do registration, then we can use the segmentation algorithm in this manner to create an objective to train against. We refer to the resulting loss function as _anatomy loss_.\n",
    "  <br>\n",
    "  \n",
    "- _Reg helps seg:_ Suppose we have a great registration algorithm that can deform one image to another in an anatomically realistic way. Then given just _one_ manually created segmentation, we have an equally great segmentaion algorithm:\n",
    "  1. Register the image you want to segment to your _atlas_, the single image for which you already have a segmentation.\n",
    "  2. Use the deformation from the registration to warp the atlas's segmentation back onto to your image.\n",
    "  \n",
    "  If we want a deep neural network to do segmentation,\n",
    "  then we can use the registration algorithm in this manner as a form of data augmentation.\n",
    "\n",
    "These ideas can be extended to the situation where our models are not so great:\n",
    " - A not-so-great segmentation model can be used to obtain a noisy version of anatomy loss, which can still help to train a registration network.\n",
    " - A not-so-great registration model can be used to create noisy segmentations from an atlas, which can still help to train a segmentation network.\n",
    "\n",
    "Each model serves as a source of weak supervision for the other, unleashing the potentially massive\n",
    "collection of unlabled images as useful training data.\n",
    "Just as\n",
    "an _atlas_ extends the usefulness of a single image segmentation,\n",
    "_DeepAtlas_ extends the usefulness of a small number of segmentations\n",
    "in the form of a weakly supervised training framework.\n",
    "At a high level, here's how it works:\n",
    "1. Construct segmentation and registration models.\n",
    "2. Use the small number of segmentations (perhaps just one!) to pre-train the segmentation model. It's okay if it's not great.\n",
    "3. Use the segmentation model as a source of weak supervision to help train the registration model.\n",
    "4. Use the registration model as a soruce of weak supervision to help train the segmentation model.\n",
    "5. Repeat steps 3 and 4 in alternation, or carry out steps 3 and 4 simultaneously in one joint training loop.\n",
    "\n",
    "The DeepAtlas method is fairly generic and does not need to prescribe specific model architectures\n",
    "or specific registration methodologies.\n",
    "In this tutorial, we use simple UNets and we do elastic registration, somewhat\n",
    "following the [paper](https://biag.cs.unc.edu/publication/dblp-confmiccai-xu-n-19/) referenced above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796e377",
   "metadata": {},
   "source": [
    "## Setup Environment and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "!python -c \"import numpy\" || pip install -q numpy\n",
    "!python -c \"import torch\" || pip install -q torch\n",
    "!python -c \"import itk\" || pip install -q itk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b9aa6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import monai\n",
    "import torch\n",
    "import itk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "import os.path\n",
    "import tempfile\n",
    "\n",
    "from utils import (\n",
    "    preview_image, preview_3D_vector_field, preview_3D_deformation,\n",
    "    jacobian_determinant, plot_against_epoch_numbers\n",
    ")\n",
    "\n",
    "monai.config.print_config()\n",
    "\n",
    "# Set deterministic training for reproducibility\n",
    "monai.utils.set_determinism(seed=2938649572)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60f8a5",
   "metadata": {},
   "source": [
    "## Load and Transform Data\n",
    "\n",
    "The data should consist of 3D images, some of which have associated segmentation labels.\n",
    "\n",
    "This tutorial was created with the OASIS-1 brain MRI dataset in mind.\n",
    "It can also generate and use [synthetic data](https://docs.monai.io/en/latest/data.html#monai.data.synthetic.create_test_image_3d).\n",
    "You can either download the OASIS data, or choose to use synthetic data as a toy demo of the code.\n",
    "\n",
    "See the [fact sheet here](https://www.oasis-brains.org/files/oasis_cross-sectional_facts.pdf)\n",
    "for information on the OASIS-1 dataset.\n",
    "\n",
    "---\n",
    "\n",
    "Note that we already have a segmentaton for each image. In the case of OASIS-1 data, these segmentations were originally done by [an algorithm](https://pubmed.ncbi.nlm.nih.gov/11293691/).\n",
    "In the case of synthetic data, true segmentations are generated for free by the process that creates the images.\n",
    "\n",
    "The situation in which DeepAtlas is interesting is one where we have many unlabled images and just a handful of labels. We will simulate this situation by pretending that only a few of the segmentation labels exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c9402",
   "metadata": {},
   "source": [
    "### Get data\n",
    "\n",
    "1. By default, this notebook will automatically download a subset of OASIS-1 data. Set the environment variable `MONAI_DATA_DIRECTORY`\n",
    "to save the data to a path of your choice.\n",
    "  - To run with the full OASIS-1 dataset, you can download the [data here](https://www.oasis-brains.org/#data).\n",
    "Make sure to select _OASIS-1_.  Download as many of the \"discs\" as you like and extract their contents into a directory called `OASIS-1`. (This notebook automatically downloads disc1, so you may already have disc1 if you have run the cells below.) Set the environment variable `MONAI_DATA_DIRECTORY`\n",
    "to point to the data directory that contains `OASIS-1`, or simply edit the variable `data_dir` below to be the path to your `OASIS-1` directory.\n",
    "<br><br>\n",
    "2. To run with synthetic data instead, set `use_synthetic_data` below to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_synthetic_data = False\n",
    "\n",
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "data_dir = os.path.join(root_dir, \"synthetic_data\" if use_synthetic_data else \"OASIS-1\")\n",
    "print(f\"Root directory: {root_dir}\")\n",
    "print(f\"Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f9bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract, if needed\n",
    "\n",
    "if not use_synthetic_data:\n",
    "    resource = \"https://download.nrg.wustl.edu/data/oasis_cross-sectional_disc1.tar.gz\"\n",
    "    md5 = \"c83e216ef8654a7cc9e2a30a4cdbe0cc\"\n",
    "\n",
    "    compressed_file = os.path.join(root_dir, \"oasis_cross-sectional_disc1.tar.gz\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        monai.apps.utils.download_and_extract(resource, compressed_file, data_dir, md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3361bcb1",
   "metadata": {},
   "source": [
    "Build lists of data paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83114737",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_synthetic_data:\n",
    "    num_segmentation_classes = 4  # background, CSF, white matter, gray matter\n",
    "\n",
    "    image_path_expression = \"PROCESSED/MPRAGE/T88_111/OAS1_*_MR*_mpr_n*_anon_111_t88_masked_gfc.img\"\n",
    "    segmentation_path_expression = \"FSL_SEG/OAS1_*_MR*_mpr_n*_anon_111_t88_masked_gfc_fseg.img\"\n",
    "\n",
    "    # Expect either of two reasonable ways of organizing extracted data:\n",
    "    # 1) <data_dir>/disc1/OAS1_0031_MR1/...\n",
    "    # 2) <data_dir>/OAS1_0031_MR1/...\n",
    "    image_paths = glob.glob(os.path.join(data_dir, '*', image_path_expression))\n",
    "    image_paths += glob.glob(os.path.join(data_dir, '*/*', image_path_expression))\n",
    "    segmentation_paths = glob.glob(os.path.join(data_dir, '*', segmentation_path_expression))\n",
    "    segmentation_paths += glob.glob(os.path.join(data_dir, '*/*', segmentation_path_expression))\n",
    "\n",
    "    # pretend that only a few segmentations are available\n",
    "    num_segs_to_select = 10\n",
    "    np.random.shuffle(segmentation_paths)\n",
    "    segmentation_paths = segmentation_paths[:num_segs_to_select]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc78f8",
   "metadata": {},
   "source": [
    "Or, instead, generate and save synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc517154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will lead to a different definition for the following variables:\n",
    "# image_paths, segmentation_paths, num_segmentation_classes\n",
    "\n",
    "if use_synthetic_data:\n",
    "    num_segmentation_classes = 2  # background, not background\n",
    "\n",
    "    save_img = monai.transforms.SaveImage(output_dir=data_dir, output_postfix=\"img\", print_log=False)\n",
    "    save_seg = monai.transforms.SaveImage(output_dir=data_dir, output_postfix=\"seg\", print_log=False)\n",
    "\n",
    "    # Set the amount of synthetic data here\n",
    "    num_img_seg_pairs_to_generate = 50\n",
    "    num_segs_to_select = 6\n",
    "\n",
    "    for i in range(num_img_seg_pairs_to_generate):\n",
    "        img, seg = monai.data.synthetic.create_test_image_3d(\n",
    "            64, 64, 64,  # image size\n",
    "            num_objs=1,\n",
    "            rad_max=30, rad_min=24,\n",
    "            noise_max=0.5,\n",
    "            num_seg_classes=num_segmentation_classes - 1,  # background is not counted\n",
    "            channel_dim=0,\n",
    "            random_state=None\n",
    "        )\n",
    "        save_img(img)\n",
    "        if i < num_segs_to_select:  # pretend that only a few segmentations are available\n",
    "            save_seg(seg)\n",
    "\n",
    "    image_paths = glob.glob(os.path.join(data_dir, \"*/*img.nii.gz\"))\n",
    "    segmentation_paths = glob.glob(os.path.join(data_dir, \"*/*seg.nii.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d2692",
   "metadata": {},
   "source": [
    "### Resize and device options\n",
    "\n",
    "Resizing is one way to keep memory usage under control.\n",
    "\n",
    "For 3D images, the demands on video memory during UNet training can be intense.\n",
    "You can reduce the batch sizes (possibly down to 1), but sometimes that is still not enough.\n",
    "Applying a resize transform can allow you to still run the notebook if you\n",
    "are otherwise running into memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to None for no resizing, or to an integer to resize all three spatial dimensions of loaded images\n",
    "resize = 96 if not use_synthetic_data else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0d794",
   "metadata": {},
   "source": [
    "Set the device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d31cc",
   "metadata": {},
   "source": [
    "### Get SRI24 atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42867917",
   "metadata": {},
   "source": [
    "Download the atlas if we don't already have it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813fb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_dir = os.path.join(root_dir, \"sri24_atlas\")\n",
    "atlas_resource = \"https://www.nitrc.org/frs/download.php/4841/sri24_spm8.zip//?i_agree=1&download_now=1\"\n",
    "atlas_md5 = \"05fbbb72d5e449fab987c7028a85a292\"\n",
    "atlas_compressed_file = os.path.join(root_dir, \"sri24_spm8.zip\")\n",
    "if not os.path.exists(atlas_dir):\n",
    "    monai.apps.utils.download_and_extract(atlas_resource, atlas_compressed_file, atlas_dir, atlas_md5)\n",
    "print(f\"Atlas directory: {atlas_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ca7be",
   "metadata": {},
   "source": [
    "Verify that the needed files are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d240287",
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_image_path = os.path.join(atlas_dir, \"templates/T1.nii\")\n",
    "atlas_csf_path = os.path.join(atlas_dir, \"tpm/csf.nii\")\n",
    "atlas_grey_path = os.path.join(atlas_dir, \"tpm/grey.nii\")\n",
    "atlas_white_path = os.path.join(atlas_dir, \"tpm/white.nii\")\n",
    "\n",
    "for p in [atlas_image_path, atlas_csf_path, atlas_grey_path, atlas_white_path]:\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Could not find {p} after supposedly downloading or checking for SRI24 atlas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f94e4",
   "metadata": {},
   "source": [
    "### Create a list of data items\n",
    "\n",
    "Each data item in `data` below is a dictionary with an `'img'` key and maybe a `'seg'` key.\n",
    "Having sometimes-missing keys is a good way to deal with partially available data--\n",
    "it can be handled nicely by MONAI transforms\n",
    "if we set `allow_missing_keys=True` in the transform parameters.\n",
    "However, missing keys will pose an additional challenge when it comes to batching data together. These issues will come up later, when we set up transforms and when we set up dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3772438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract an image or segmentation ID from its path\n",
    "def path_to_id(path):\n",
    "    if use_synthetic_data:\n",
    "        return os.path.basename(path).split('_')[0]\n",
    "    else:\n",
    "        return os.path.basename(path).strip('OAS1_')[:8]\n",
    "\n",
    "\n",
    "seg_ids = list(map(path_to_id, segmentation_paths))\n",
    "img_ids = map(path_to_id, image_paths)\n",
    "data = []\n",
    "for img_index, img_id in enumerate(img_ids):\n",
    "    data_item = {'img': image_paths[img_index]}\n",
    "    if img_id in seg_ids:\n",
    "        data_item['seg'] = segmentation_paths[seg_ids.index(img_id)]\n",
    "    data.append(data_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab9d7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the following to preview a random image\n",
    "# data_item = random.choice(data)\n",
    "# preview_image(monai.transforms.LoadImage(image_only=True)(data_item['img']), figsize=(6,6), normalize_by=\"slice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812cde6",
   "metadata": {},
   "source": [
    "### Datasets for segmentation network pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07883a",
   "metadata": {},
   "source": [
    "It is recommended that the segmentation network be pre-trained using whatever\n",
    "little segmentation data is available,\n",
    "before the alternate/joint training begins.\n",
    "\n",
    "We select the subset of `data` that has segmentation labels available, and then split that into a training set and a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seg_available = list(filter(lambda d: 'seg' in d.keys(), data))\n",
    "data_seg_unavailable = list(filter(lambda d: 'seg' not in d.keys(), data))\n",
    "\n",
    "data_seg_available_train, data_seg_available_valid = \\\n",
    "    monai.data.utils.partition_dataset(data_seg_available, ratios=(8, 2))\n",
    "# Validation of the segmentation network only makes sense if you have enough segmentation labels.\n",
    "# E.g. definitely skip validation here if there's just one segmentation label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e12dc4c",
   "metadata": {},
   "source": [
    "Next we set up the chain of transforms that will be used to load images and segmentations for the pre-training of the segmentation network.\n",
    "\n",
    "<a id=\"augment\"></a>\n",
    "We'd like to keep this part of the tutorial simple, but this would be a good place to introduce data augmentation. Feel free to explore the random transforms that show up in the [documentation](https://docs.monai.io/en/stable/transforms.html) and go to town! Make sure any randomizable transforms appear _after_ the deterministic ones, for caching purposes. Make sure the interpolation mode for any transforms of segmentations is `\"nearest\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_seg_available = monai.transforms.Compose(\n",
    "    transforms=[\n",
    "        monai.transforms.LoadImageD(keys=['img', 'seg'], image_only=True),\n",
    "        monai.transforms.ToTensorD(keys=['img', 'seg']),\n",
    "        monai.transforms.TransposeD(keys=['img', 'seg'], indices=(2, 1, 0)),\n",
    "        monai.transforms.AddChannelD(keys=['img', 'seg']),\n",
    "        monai.transforms.ResizeD(\n",
    "            keys=['img', 'seg'],\n",
    "            spatial_size=(resize, resize, resize),\n",
    "            mode=['trilinear', 'nearest'],\n",
    "            align_corners=[False, None]\n",
    "        ) if resize is not None else monai.transforms.Identity()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Supress the many warnings related to depracation of the Analyze file format\n",
    "# (without this, we would see warnings when the LoadImage transform calls itk to load Analyze files)\n",
    "itk.ProcessObject.SetGlobalWarningDisplay(False)\n",
    "\n",
    "# Uncomment the following lines to preview a random image with the transform above applied\n",
    "# data_item = transform_seg_available(random.choice(data_seg_available))\n",
    "# preview_image(data_item['img'][0])\n",
    "# preview_image(data_item['seg'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961cfcf1",
   "metadata": {},
   "source": [
    "And now we define the datasets that use those transforms to load the data. We use `CacheDataset` to take advantage of MONAI's caching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_seg_available_train = monai.data.CacheDataset(\n",
    "    data=data_seg_available_train,\n",
    "    transform=transform_seg_available,\n",
    "    cache_num=16\n",
    ")\n",
    "\n",
    "dataset_seg_available_valid = monai.data.CacheDataset(\n",
    "    data=data_seg_available_valid,\n",
    "    transform=transform_seg_available,\n",
    "    cache_num=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177734b2",
   "metadata": {},
   "source": [
    "At this point we have everything we need to construct the segmentation network and pre-train it.\n",
    "If you want to get right to it, feel free to skip to\n",
    "[Create Segmentation Network](#Create-Segmentation-Network)\n",
    "and then to\n",
    "[Pre-train segmentation network](#Pre-train-segmentation-network).\n",
    "Continue reading in order to see more datasets constructed for the later phase of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7edd6",
   "metadata": {},
   "source": [
    "### Datasets for training both registration and segmentation networks\n",
    "\n",
    "For the joint/alternative training of the registration and segmentation networks, we want to load _pairs_ of images, along with their segmentation labels when those are available. \n",
    "\n",
    "We reserve some images for validation of the registration network, then we create data lists for _pairs_ of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the joint/alternating training process, we will use reuse data_seg_available_valid\n",
    "# for validating the segmentation network.\n",
    "# So we should not let the registration or segmentation networks see these images in training.\n",
    "data_without_seg_valid = data_seg_unavailable + data_seg_available_train  # Note the order\n",
    "\n",
    "# For validation of the registration network, we prefer not to use the precious data_seg_available_train,\n",
    "# if that's possible. The following split tries to use data_seg_unavailable for the\n",
    "# the validation set, to the extent possible.\n",
    "data_valid, data_train = monai.data.utils.partition_dataset(\n",
    "    data_without_seg_valid,  # Note the order\n",
    "    ratios=(2, 8),  # Note the order\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "def take_data_pairs(data, symmetric=True):\n",
    "    \"\"\"Given a list of dicts that have keys for an image and maybe a segmentation,\n",
    "    return a list of dicts corresponding to *pairs* of images and maybe segmentations.\n",
    "    Pairs consisting of a repeated image are not included.\n",
    "    If symmetric is set to True, then for each pair that is included, its reverse is also included\"\"\"\n",
    "    data_pairs = []\n",
    "    for i in range(len(data)):\n",
    "        j_limit = len(data) if symmetric else i\n",
    "        for j in range(j_limit):\n",
    "            if j == i:\n",
    "                continue\n",
    "            d1 = data[i]\n",
    "            d2 = data[j]\n",
    "            pair = {\n",
    "                'img1': d1['img'],\n",
    "                'img2': d2['img']\n",
    "            }\n",
    "            if 'seg' in d1.keys():\n",
    "                pair['seg1'] = d1['seg']\n",
    "            if 'seg' in d2.keys():\n",
    "                pair['seg2'] = d2['seg']\n",
    "            data_pairs.append(pair)\n",
    "    return data_pairs\n",
    "\n",
    "\n",
    "data_pairs_valid = take_data_pairs(data_valid)\n",
    "data_pairs_train = take_data_pairs(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc4dfb",
   "metadata": {},
   "source": [
    "<a id=\"subdivide\"></a>\n",
    "At this point the data items in `data_pairs_train` and `data_pairs_valid` all have `img1` and `img2` keys, but only some of them have `seg1` or `seg2` keys. \n",
    "\n",
    "The variable presence of the `seg1` and `seg2` keys poses a problem for the collation of data into batches.\n",
    "If we want a batch size greater than 1, then we need to address this.\n",
    "Our approach is to subdivide the lists of data pairs based on the availability of `seg1` and `seg2`.\n",
    "The naming convention will be as follows:\n",
    "\n",
    "| key | segmentation availability |\n",
    "| ----------- | ----------- |\n",
    "| `00`      | no segs available |\n",
    "| `01`   | seg2 available, but no seg1 |\n",
    "| `10`   | seg1 available, but no seg2 |\n",
    "| `11`   | both segs available |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subdivide_list_of_data_pairs(data_pairs_list):\n",
    "    out_dict = {'00': [], '01': [], '10': [], '11': []}\n",
    "    for d in data_pairs_list:\n",
    "        if 'seg1' in d.keys() and 'seg2' in d.keys():\n",
    "            out_dict['11'].append(d)\n",
    "        elif 'seg1' in d.keys():\n",
    "            out_dict['10'].append(d)\n",
    "        elif 'seg2' in d.keys():\n",
    "            out_dict['01'].append(d)\n",
    "        else:\n",
    "            out_dict['00'].append(d)\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "data_pairs_valid_subdivided = subdivide_list_of_data_pairs(data_pairs_valid)\n",
    "data_pairs_train_subdivided = subdivide_list_of_data_pairs(data_pairs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a020fc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print some useful counts to be aware of\n",
    "\n",
    "num_train_reg_net = len(data_pairs_train)\n",
    "num_valid_reg_net = len(data_pairs_valid)\n",
    "num_train_both = len(data_pairs_train_subdivided['01']) +\\\n",
    "    len(data_pairs_train_subdivided['10']) +\\\n",
    "    len(data_pairs_train_subdivided['11'])\n",
    "\n",
    "\n",
    "print(f\"\"\"We have {num_train_both} pairs to train reg_net and seg_net together,\n",
    "  and an additional {num_train_reg_net - num_train_both} to train reg_net alone.\"\"\")\n",
    "print(f\"We have {num_valid_reg_net} pairs for reg_net validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a02a29",
   "metadata": {},
   "source": [
    "Now we set up the chain of transforms that will be used for loading image pairs.\n",
    "\n",
    "We will concatenate the \"fixed\" and \"moving\" images along the channel dimension.\n",
    "\n",
    "Again this would be a good place to introduce data augmentation-- see the comment [above](#augment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pair = monai.transforms.Compose(\n",
    "    transforms=[\n",
    "        monai.transforms.LoadImageD(keys=['img1', 'seg1', 'img2', 'seg2'], image_only=True, allow_missing_keys=True),\n",
    "        monai.transforms.ToTensorD(keys=['img1', 'seg1', 'img2', 'seg2'], allow_missing_keys=True),\n",
    "        monai.transforms.TransposeD(keys=['img1', 'seg1', 'img2', 'seg2'], indices=(2, 1, 0), allow_missing_keys=True),\n",
    "        monai.transforms.AddChannelD(keys=['img1', 'seg1', 'img2', 'seg2'], allow_missing_keys=True),\n",
    "        monai.transforms.ConcatItemsD(keys=['img1', 'img2'], name='img12', dim=0),\n",
    "        monai.transforms.DeleteItemsD(keys=['img1', 'img2']),\n",
    "        monai.transforms.ResizeD(\n",
    "            keys=['img12', 'seg1', 'seg2'],\n",
    "            spatial_size=(resize, resize, resize),\n",
    "            mode=['trilinear', 'nearest', 'nearest'],\n",
    "            allow_missing_keys=True,\n",
    "            align_corners=[False, None, None]\n",
    "        ) if resize is not None else monai.transforms.Identity()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af437ab6",
   "metadata": {},
   "source": [
    "And now we define the datasets that use the transforms to load the data. Again we use `CacheDataset` to take advantage of MONAI's caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec657e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are dictionaries that map segmentation availability labels 00,10,01,11 to MONAI datasets\n",
    "\n",
    "dataset_pairs_train_subdivided = {\n",
    "    seg_availability: monai.data.CacheDataset(\n",
    "        data=data_list,\n",
    "        transform=transform_pair,\n",
    "        cache_num=32\n",
    "    )\n",
    "    for seg_availability, data_list in data_pairs_train_subdivided.items()\n",
    "}\n",
    "\n",
    "\n",
    "dataset_pairs_valid_subdivided = {\n",
    "    seg_availability: monai.data.CacheDataset(\n",
    "        data=data_list,\n",
    "        transform=transform_pair,\n",
    "        cache_num=32\n",
    "    )\n",
    "    for seg_availability, data_list in data_pairs_valid_subdivided.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2fb37a",
   "metadata": {},
   "source": [
    "To help keep things straight, here is a guide to the naming convention we have used for the various objects that represent data:\n",
    "\n",
    "| variable name component | suggested meaning |\n",
    "| ----------- | ----------- |\n",
    "| `data_`      | data items are dictionaries with keys mapping to filepaths |\n",
    "| `dataset_`   | a MONAI dataset; getting an item loads images and applies transforms |\n",
    "| `dataloader_`   | a MONAI dataloader; items from dataset are collated into batches |\n",
    "| `_train_`   | data is meant for training |\n",
    "| `_valid_`   | data is meant for validation |\n",
    "| `_seg_available_`   | ground truth segmentations are available |\n",
    "| `_seg_unavailable_`   | ground truth segmentations are not available |\n",
    "| `_pairs_`   | data consists of ordered pairs of images and potentially segmentations |\n",
    "| `_subdivided`   | a mapping from segmentation availability labels `00,10,01,11` to the suggested type of object |\n",
    "\n",
    "(We have not defined any dataloaders yet, but we will soon enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70210c5e",
   "metadata": {},
   "source": [
    "### Load SRI24 atlas and probability maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b53e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "atlas_transform = monai.transforms.Compose(transforms=[\n",
    "    monai.transforms.LoadImage(reader=\"itkreader\", image_only=True),\n",
    "    monai.transforms.ToTensor(),\n",
    "    monai.transforms.Transpose(indices=(2, 1, 0)),\n",
    "    monai.transforms.AddChannel(),\n",
    "    monai.transforms.Resize(\n",
    "        spatial_size=(resize, resize, resize),\n",
    "        mode='trilinear',\n",
    "        align_corners=False\n",
    "    ) if resize is not None else monai.transforms.Identity()\n",
    "])\n",
    "\n",
    "# Load atlas image\n",
    "atlas_image = atlas_transform(atlas_image_path).to(device)\n",
    "\n",
    "# Load probability maps\n",
    "atlas_csf = atlas_transform(atlas_csf_path)\n",
    "atlas_grey = atlas_transform(atlas_grey_path)\n",
    "atlas_white = atlas_transform(atlas_white_path)\n",
    "\n",
    "# The sum of the probability maps should max at 1, because the possibilities csf, grey, and white are mutually\n",
    "# exclusive, and because we expect that there exists a voxel that is certain to be one of them\n",
    "# (i.e. not background), for example any voxel that is well inside the brain region.\n",
    "normalizing_factor = 1 / (atlas_csf + atlas_grey + atlas_white).max()\n",
    "atlas_csf *= normalizing_factor\n",
    "atlas_grey *= normalizing_factor\n",
    "atlas_white *= normalizing_factor\n",
    "atlas_background = 1. - atlas_csf - atlas_grey - atlas_white\n",
    "\n",
    "# Create segmentation as a [4,H,W,D] shape tensor\n",
    "atlas_seg = torch.cat([atlas_background, atlas_csf, atlas_grey, atlas_white], dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_image(atlas_seg.argmax(dim=0).cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a37acf",
   "metadata": {},
   "source": [
    "There are voxels well within the brain region that have been marked as \"most likely background.\" This doesn't make any sense\n",
    "\n",
    "We could try and fix this using bayesian thinking. Suppose you give me a voxel and tell me that the probability that it's bg, csf, grey, white respectively is $[p,q,r,s]$. Now suppose I look at where this voxel lives and see that it's well within the brain region, so I decide that there's absolutely no way it's background. How do I update my belief from $[p,q,r,s]$? It's simply $[0,\\frac{q}{q+r+s}, \\frac{r}{q+r+s}, \\frac{s}{q+r+s}]$. For example \n",
    "$$P(\\text{csf}\\ |\\ \\text{not bg}) = \\frac{P(\\text{csf and not bg})}{P(\\text{not bg})} = \\frac{P(\\text{csf})}{P(\\text{not bg})} = \\frac{q}{q+r+s}.$$\n",
    "Before going there, I think I should study the SRI24 paper a little more closely to see what they originally meant by \"probability maps.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736a2c6",
   "metadata": {},
   "source": [
    "## Create Segmentation Network\n",
    "\n",
    "We use a [MONAI UNet](https://docs.monai.io/en/stable/networks.html?highlight=unet#unet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_net = monai.networks.nets.UNet(\n",
    "    3,  # spatial dims\n",
    "    1,  # input channels\n",
    "    num_segmentation_classes,  # output channels\n",
    "    (8, 16, 16, 32, 32, 64, 64),  # channel sequence\n",
    "    (1, 2, 1, 2, 1, 2),  # convolutional strides\n",
    "    dropout=0.2,\n",
    "    norm='batch'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc87f1",
   "metadata": {},
   "source": [
    "A couple of differences compared to the architecture used in the paper:\n",
    "\n",
    "- We downsample by using convolution with stride $>1$, rather than maxpooling.\n",
    "- We use PReLU activation, rather than leaky ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e6f8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try out a forward pass\n",
    "\n",
    "data_item = random.choice(dataset_seg_available_train)\n",
    "seg_net_example_output = seg_net(data_item['img'].unsqueeze(0))\n",
    "print(f\"Segmentation classes: {torch.unique(data_item['seg'])}\")\n",
    "print(f\"Shape of ground truth label: {data_item['seg'].unsqueeze(0).shape}\")\n",
    "print(f\"Shape of seg_net output: {seg_net_example_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b6d54",
   "metadata": {},
   "source": [
    "### Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef665ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = monai.losses.DiceLoss(\n",
    "    include_background=True,\n",
    "    to_onehot_y=True,  # Our seg labels are single channel images indicating class index, rather than one-hot\n",
    "    softmax=True,  # Note that our segmentation network is missing the softmax at the end\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "# A version of the dice loss with to_onehot_y=False and softmax=False;\n",
    "# This will be handy for anatomy loss, for which we often compare two outputs of seg_net\n",
    "dice_loss2 = monai.losses.DiceLoss(\n",
    "    include_background=True,\n",
    "    to_onehot_y=False,\n",
    "    softmax=False,\n",
    "    reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try out the two versions of dice loss on the example. The two values printed should be the same.\n",
    "\n",
    "# true_seg = data_item['seg'].unsqueeze(0)\n",
    "# print(\n",
    "#     dice_loss(\n",
    "#         seg_net_example_output, # Prediction from seg_net, as logits\n",
    "#         true_seg # Ground truth segmentation, as class labels rather than one-hot\n",
    "#     ),\n",
    "#     dice_loss2(\n",
    "#         seg_net_example_output.softmax(dim=1), # Prediction from seg_net, as probabilities\n",
    "#         monai.networks.one_hot(true_seg,\n",
    "#                                num_classes=num_segmentation_classes) # Ground truth segmentation, one-hot encoded\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378dbb89",
   "metadata": {},
   "source": [
    "Feel free to skip to [Pre-train segmentation network](#Pre-train-segmentation-network) and then return here, or\n",
    "continue reading in order to see the registration network prepared for the later phase of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37debcd7",
   "metadata": {},
   "source": [
    "## Create Registration Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38de75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar architecture to the one used in the paper\n",
    "reg_net = monai.networks.nets.UNet(\n",
    "    3,  # spatial dims\n",
    "    2,  # input channels (one for fixed image and one for moving image)\n",
    "    3,  # output channels (to represent 3D displacement vector field)\n",
    "    (16, 32, 32, 32, 32),  # channel sequence\n",
    "    (1, 2, 2, 2),  # convolutional strides\n",
    "    dropout=0.2,\n",
    "    norm=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff16a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try out a forward pass\n",
    "\n",
    "def take_random_from_subdivided_dataset(dataset_subdivided):\n",
    "    \"\"\"Given a dict mapping segmentation availability labels to datasets, return a random data item\"\"\"\n",
    "    datasets = list(dataset_subdivided.values())\n",
    "    datasets_combined = sum(datasets[1:], datasets[0])\n",
    "    return random.choice(datasets_combined)\n",
    "\n",
    "\n",
    "data_item = take_random_from_subdivided_dataset(dataset_pairs_train_subdivided)\n",
    "reg_net_example_input = data_item['img12'].unsqueeze(0)\n",
    "reg_net_example_output = reg_net(reg_net_example_input)\n",
    "print(f\"Shape of reg_net input: {reg_net_example_input.shape}\")\n",
    "print(f\"Shape of reg_net output: {reg_net_example_output.shape}\")\n",
    "image_scale = reg_net_example_input.shape[-1]  # comes in handy later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9457acb5",
   "metadata": {},
   "source": [
    "We will interpret the output of `reg_net` as a displacement vector field.\n",
    "\n",
    "However, this is just one way to use a UNet for registration.\n",
    "The framework here is agnostic to the exact way in which the output of `reg_net` is interpreted,\n",
    "as long is there is some differentiable way to combine the output of `reg_net` with an image and obtain\n",
    "a warped image. One could, for example, interpret the output of `reg_net` as a velocity vector field, and then deform an image by flowing it along the vector field, as is done in fluid based registration.\n",
    "The possibilities are endless.\n",
    "\n",
    "To keep things simple, we choose a fairly direct interpretation: the output of `reg_net`\n",
    "gives us the three components of a 3D displacement vector field.\n",
    "A zero output from `reg_net`, for example, would result in the identity warping.\n",
    "The 3-vector at each voxel tells us how many voxels to move in each direction,\n",
    "and all together this defines a mapping from a subset of 3-space (the image domain) to 3-space.\n",
    "\n",
    "In the MONAI documentation, you will see the term _dense displacement field (DDF)_ used to\n",
    "refer to a 3-channel tensor interpreted in this way.\n",
    "\n",
    "### Image warping\n",
    "\n",
    "We now define `warp`, the object that applies a DDF to an image and yields a warped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For warping segmentation labels or class predictions, we sometimes want to use nearest neighbor interpolation\n",
    "# Note that \"bilinear\" means \"trilinear\" in the case of 3D images\n",
    "warp = monai.networks.blocks.Warp(mode=\"bilinear\", padding_mode=\"border\")\n",
    "warp_nearest = monai.networks.blocks.Warp(mode=\"nearest\", padding_mode=\"border\")\n",
    "\n",
    "# Use example reg_net output to apply warp\n",
    "example_warped_image = warp(\n",
    "    data_item['img12'][[1], :, :, :].unsqueeze(0),  # moving image\n",
    "    reg_net_example_output  # warping\n",
    ")\n",
    "\n",
    "# Uncomment to preview warped image from forward pass example above\n",
    "# preview_image(example_warped_image[0,0].detach())\n",
    "\n",
    "# Uncomment to preview displacement field from forward pass example above\n",
    "# preview_3D_vector_field(reg_net_example_output.detach()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb847529",
   "metadata": {},
   "source": [
    "<a id=\"warp_warning\"></a>\n",
    "We will need to be careful about when to use `warp` and when to use `warp_nearest`, especially with segmentation labels. Potential pitfalls:\n",
    "- Using _trilinear interpolation_ to warp a segmentation with one channel containing class labels is a disaster: interpolating among labels would lead to intermediate values that aren't labels!\n",
    "- Using _nearest neighbor inteprolation_ to warp a segmentation results in a reasonable output, but it is not a differentiable operation with respect to the deformation vector field (or, rather, derivatives vanish almost everywhere). This can be problematic for training, depending on what one is trying to do.\n",
    "- Warping a segmentation with `padding_mode=\"zeros\"` can pull in zero vectors, which might not make sense depending on how the segmentation is encoded. This is why we chose `padding_mode=\"border\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b681022",
   "metadata": {},
   "source": [
    "### Image similarity loss\n",
    "\n",
    "To measure the similarity between target images and warped moving images, we use local normalized cross-correlation loss.\n",
    "\n",
    "(The paper works with global NCC, but MONAI provides local NCC and it's easy to use.)\n",
    "\n",
    "(Another slight difference: this loss function returns `-LNCC`, not `1-LNCC`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncc_loss = monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
    "    spatial_dims=3,\n",
    "    kernel_size=3,\n",
    "    kernel_type='rectangular',\n",
    "    reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try out the image similarity loss on the example.\n",
    "\n",
    "# lncc_loss(\n",
    "#     example_warped_image, # registered image\n",
    "#     data_item['img12'][[0],:,:,:].unsqueeze(0) # target (\"fixed image\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da58e16",
   "metadata": {},
   "source": [
    "### Regularization loss\n",
    "\n",
    "One of the greatest challenges in non-parameteric image registration is producing sufficiently smooth deformations. If we allow our model to propose deformations that vary chaotically over space,\n",
    "it will happily do so in order to get images to match as closely as it can muster.\n",
    "\n",
    "We use bending energy loss for regularization.\n",
    "\n",
    "This type of image registration is referred to as _elastic_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bending_loss = monai.losses.BendingEnergyLoss()\n",
    "\n",
    "# Uncomment to try out the bending energy loss on the example\n",
    "# bending_loss(reg_net_example_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6b085",
   "metadata": {},
   "source": [
    "## Train Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb326a",
   "metadata": {},
   "source": [
    "Two training cells can be found in this notebook:\n",
    "- one for pre-training `seg_net` alone, and\n",
    "- one for alternately training `reg_net` and `seg_net` together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc030353",
   "metadata": {},
   "source": [
    "### Pre-train segmentation network\n",
    "\n",
    "We now pre-train the segmentation network, using whatever little segmentation data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72730113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataloaders\n",
    "\n",
    "dataloader_seg_available_train = monai.data.DataLoader(\n",
    "    dataset_seg_available_train,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_seg_available_valid = monai.data.DataLoader(\n",
    "    dataset_seg_available_valid,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc25b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training cell\n",
    "# (if already done then you may skip to and uncomment the checkpoint loading cell below)\n",
    "\n",
    "seg_net.to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(seg_net.parameters(), learning_rate)\n",
    "\n",
    "max_epochs = 60\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "val_interval = 5\n",
    "\n",
    "for epoch_number in range(max_epochs):\n",
    "\n",
    "    print(f\"Epoch {epoch_number+1}/{max_epochs}:\")\n",
    "\n",
    "    seg_net.train()\n",
    "    losses = []\n",
    "    for batch in dataloader_seg_available_train:\n",
    "        imgs = batch['img'].to(device)\n",
    "        true_segs = batch['seg'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_segs = seg_net(imgs)\n",
    "        loss = dice_loss(predicted_segs, true_segs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    training_loss = np.mean(losses)\n",
    "    print(f\"\\ttraining loss: {training_loss}\")\n",
    "    training_losses.append([epoch_number, training_loss])\n",
    "\n",
    "    if epoch_number % val_interval == 0:\n",
    "        seg_net.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader_seg_available_valid:\n",
    "                imgs = batch['img'].to(device)\n",
    "                true_segs = batch['seg'].to(device)\n",
    "                predicted_segs = seg_net(imgs)\n",
    "                loss = dice_loss(predicted_segs, true_segs)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        validation_loss = np.mean(losses)\n",
    "        print(f\"\\tvalidation loss: {validation_loss}\")\n",
    "        validation_losses.append([epoch_number, validation_loss])\n",
    "\n",
    "# Free up some memory\n",
    "del loss, predicted_segs, true_segs, imgs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses\n",
    "\n",
    "plot_against_epoch_numbers(training_losses, label=\"training\")\n",
    "plot_against_epoch_numbers(validation_losses, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.ylabel('mean dice loss')\n",
    "plt.title('seg_net pretraining')\n",
    "plt.savefig('seg_net_pretrained_losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d031795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; SAVE\n",
    "torch.save(seg_net.state_dict(), 'seg_net_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfab83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; LOAD\n",
    "# seg_net.load_state_dict(torch.load('seg_net_pretrained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to try out the pretrained seg net on a random validation image\n",
    "\n",
    "seg_net.to(device)\n",
    "\n",
    "data_item = random.choice(dataset_seg_available_valid)\n",
    "test_input = data_item['img']\n",
    "test_seg_true = data_item['seg']\n",
    "seg_net.eval()\n",
    "with torch.no_grad():\n",
    "    test_seg_predicted = seg_net(test_input.unsqueeze(0).cuda()).cpu()\n",
    "\n",
    "print(\"Original image from validation set:\")\n",
    "preview_image(test_input[0])\n",
    "print(\"Ground truth segmentation:\")\n",
    "preview_image(test_seg_true[0])\n",
    "print(\"Our predicted segmentation:\")\n",
    "preview_image(torch.argmax(torch.softmax(test_seg_predicted, dim=1), dim=1, keepdim=True)[0, 0])\n",
    "\n",
    "del test_seg_predicted\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ef58f",
   "metadata": {},
   "source": [
    "### Train both networks in alternation\n",
    "\n",
    "We can train `reg_net` and `seg_net` *jointly*, or *in alternation*.\n",
    "We choose to do the latter because it conserves memory.\n",
    "In the case of brain MRI data, the forward pass for even a single batch can be very demanding on memory.\n",
    "\n",
    "Note that this training is not adversarial.\n",
    "Neither of the two networks is directly penalized for the improved performance of the other network.\n",
    "Each network merely provides a noisy \"ground truth\" for the other to train from.\n",
    "We do not expect the sort of instability that often plagues adversarial training and that makes alternative training less viable. \n",
    "\n",
    "#### Prepare components to use in the training loop\n",
    "\n",
    "While training one network, we don't want to compute gradients for the other. The following function streamlines this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0700cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_training(network_to_train, network_to_not_train):\n",
    "    \"\"\"\n",
    "        Switch out of training one network and into training another\n",
    "    \"\"\"\n",
    "\n",
    "    for param in network_to_not_train.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in network_to_train.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    network_to_not_train.eval()\n",
    "    network_to_train.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e94ffd",
   "metadata": {},
   "source": [
    "Next, following the paper, we define regularization loss, similarity loss, and anatomy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f96c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_loss = bending_loss\n",
    "\n",
    "\n",
    "def similarity_loss(displacement_field, image_pair):\n",
    "    \"\"\" Accepts a batch of displacement fields, shape (B,3,H,W,D),\n",
    "        and a batch of image pairs, shape (B,2,H,W,D). \"\"\"\n",
    "    warped_img2 = warp(image_pair[:, [1], :, :, :], displacement_field)\n",
    "    return lncc_loss(\n",
    "        warped_img2,  # prediction\n",
    "        image_pair[:, [0], :, :, :]  # target\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d93aee",
   "metadata": {},
   "source": [
    "Defining anatomy loss involves computing a warped segmentation,\n",
    "and in order to train `reg_net` the warping operation must be differentiable with respect to the deformation\n",
    "vector field.\n",
    "For this reason, we use `warp` and not `warp_nearest` in the definition of `anatomy_loss` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anatomy_loss(displacement_field, image_pair, seg_net, gt_seg1=None, gt_seg2=None):\n",
    "    \"\"\"\n",
    "    Accepts a batch of displacement fields, shape (B,3,H,W,D),\n",
    "    and a batch of image pairs, shape (B,2,H,W,D).\n",
    "    seg_net is the model used to segment an image,\n",
    "      mapping (B,1,H,W,D) to (B,C,H,W,D) where C is the number of segmentation classes.\n",
    "    gt_seg1 and gt_seg2 are ground truth segmentations for the images in image_pair, if ground truth is available;\n",
    "      if unavailable then they can be None.\n",
    "      gt_seg1 and gt_seg2 are expected to be in the form of class labels, with shape (B,1,H,W,D).\n",
    "    \"\"\"\n",
    "    if gt_seg1 is not None:\n",
    "        # ground truth seg of target image\n",
    "        seg1 = monai.networks.one_hot(\n",
    "            gt_seg1, num_segmentation_classes\n",
    "        )\n",
    "    else:\n",
    "        # seg_net on target image, \"noisy ground truth\"\n",
    "        seg1 = seg_net(image_pair[:, [0], :, :, :]).softmax(dim=1)\n",
    "\n",
    "    if gt_seg2 is not None:\n",
    "        # ground truth seg of moving image\n",
    "        seg2 = monai.networks.one_hot(\n",
    "            gt_seg2, num_segmentation_classes\n",
    "        )\n",
    "    else:\n",
    "        # seg_net on moving image, \"noisy ground truth\"\n",
    "        seg2 = seg_net(image_pair[:, [1], :, :, :]).softmax(dim=1)\n",
    "\n",
    "    # seg1 and seg2 are now in the form of class probabilities at each voxel\n",
    "    # The trilinear interpolation of the function `warp` is then safe to use;\n",
    "    # it will preserve the probabilistic interpretation of seg2.\n",
    "\n",
    "    return dice_loss2(\n",
    "        warp(seg2, displacement_field),  # warp of moving image segmentation\n",
    "        seg1  # target image segmentation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for forward pass of reg_net, to avoid duplicating code between training and validation\n",
    "\n",
    "def reg_losses(batch):\n",
    "    img12 = batch['img12'].to(device)\n",
    "\n",
    "    displacement_field12 = reg_net(img12)\n",
    "\n",
    "    loss_sim = similarity_loss(displacement_field12, img12)\n",
    "\n",
    "    loss_reg = regularization_loss(displacement_field12)\n",
    "\n",
    "    gt_seg1 = batch['seg1'].to(device) if 'seg1' in batch.keys() else None\n",
    "    gt_seg2 = batch['seg2'].to(device) if 'seg2' in batch.keys() else None\n",
    "    loss_ana = anatomy_loss(displacement_field12, img12, seg_net, gt_seg1, gt_seg2)\n",
    "\n",
    "    return loss_sim, loss_reg, loss_ana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b8bf86",
   "metadata": {},
   "source": [
    "Here is a flowchart summarizing the final loss computation for `reg_net`, where\n",
    "dotted lines indicate data that is sometimes not available:\n",
    "\n",
    "![registration network loss flowchart](notebook_resources/reg_loss_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba879ab",
   "metadata": {},
   "source": [
    "Recall from [above](#subdivide) that we have a dataset for each \"segmentation availability label\" from `00,10,01,11`. The purpose of this subdivision is to allow data to be collated into tensors with a batch dimension.\n",
    "\n",
    "Then we must have a four dataloaders, one for each segmentation availability label.\n",
    "Four for `reg_net` training, and four for `reg_net` validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11402c6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The following are dictionaries that map segmentation availability labels 00,10,01,11 to MONAI dataloaders\n",
    "\n",
    "dataloader_pairs_train_subdivided = {\n",
    "    seg_availability: monai.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        num_workers=4,\n",
    "        shuffle=True\n",
    "    )\n",
    "    if len(dataset) > 0 else []  # empty dataloaders are not a thing-- put an empty list if needed\n",
    "    for seg_availability, dataset in dataset_pairs_train_subdivided.items()\n",
    "}\n",
    "\n",
    "dataloader_pairs_valid_subdivided = {\n",
    "    seg_availability: monai.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=4,\n",
    "        num_workers=4,\n",
    "        shuffle=True  # Shuffle validation data because we will only take a sample for validation each time\n",
    "    )\n",
    "    if len(dataset) > 0 else []  # empty dataloaders are not a thing-- put an empty list if needed\n",
    "    for seg_availability, dataset in dataset_pairs_valid_subdivided.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461df981",
   "metadata": {},
   "source": [
    "Having four dataloaders can make the training loop look unnecessarily complicated.\n",
    "\n",
    "To keep the training loop simple, we will create generators that can run all four dataloaders\n",
    "and sample them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724379ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_availabilities = ['00', '01', '10', '11']\n",
    "\n",
    "\n",
    "def create_batch_generator(dataloader_subdivided, weights=None):\n",
    "    \"\"\"\n",
    "    Create a batch generator that samples data pairs with various segmentation availabilities.\n",
    "\n",
    "    Arguments:\n",
    "        dataloader_subdivided : a mapping from the labels in seg_availabilities to dataloaders\n",
    "        weights : a list of probabilities, one for each label in seg_availabilities;\n",
    "                  if not provided then we weight by the number of data items of each type,\n",
    "                  effectively sampling uniformly over the union of the datasets\n",
    "\n",
    "    Returns: batch_generator\n",
    "        A function that accepts a number of batches to sample and that returns a generator.\n",
    "        The generator will weighted-randomly pick one of the seg_availabilities and\n",
    "        yield the next batch from the corresponding dataloader.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.array([len(dataloader_subdivided[s]) for s in seg_availabilities])\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    dataloader_subdivided_as_iterators = {s: iter(d) for s, d in dataloader_subdivided.items()}\n",
    "\n",
    "    def batch_generator(num_batches_to_sample):\n",
    "        for _ in range(num_batches_to_sample):\n",
    "            seg_availability = np.random.choice(seg_availabilities, p=weights)\n",
    "            try:\n",
    "                yield next(dataloader_subdivided_as_iterators[seg_availability])\n",
    "            except StopIteration:  # If dataloader runs out, restart it\n",
    "                dataloader_subdivided_as_iterators[seg_availability] =\\\n",
    "                    iter(dataloader_subdivided[seg_availability])\n",
    "                yield next(dataloader_subdivided_as_iterators[seg_availability])\n",
    "    return batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdacfa6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_generator_train_reg = create_batch_generator(dataloader_pairs_train_subdivided)\n",
    "batch_generator_valid_reg = create_batch_generator(dataloader_pairs_valid_subdivided)\n",
    "\n",
    "\n",
    "# When training seg_net alone, we only consider data pairs for which at least one ground truth seg is available\n",
    "seg_train_sampling_weights = [0] + [len(dataloader_pairs_train_subdivided[s]) for s in seg_availabilities[1:]]\n",
    "print(f\"\"\"When training seg_net alone, segmentation availabilities {seg_availabilities}\n",
    "will be sampled with respective weights {seg_train_sampling_weights}\"\"\")\n",
    "batch_generator_train_seg = create_batch_generator(dataloader_pairs_train_subdivided, seg_train_sampling_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95101e5",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "\n",
    "Finally, we train `reg_net` and `seg_net` in alternation.\n",
    "\n",
    "We are abusing the terminology \"epoch\" in the training cell below.\n",
    "When training `reg_net`, we don't run through _all_ the data in each iteration of this loop.\n",
    "There are a lot of data pairs, and running through all pairs time consuming.\n",
    "\"Epoch\" here is just a convenient name for \n",
    "> _one iteration of the loop in which we train `reg_net` on\n",
    "a bunch of image pairs, train `seg_net` on a bunch of image pairs, possibly compute some performance metrics, and log some info_\n",
    "\n",
    "It's a misnomer.\n",
    "\n",
    "---\n",
    "\n",
    "We save the models with the best validation metrics,\n",
    "but we use the final models, not the best models, for visualization at the end of this demo. It is nice to visualize the performance of `reg_net`and `seg_net` during the same epoch.\n",
    "\n",
    "---\n",
    "\n",
    "We discussed the loss computation for `reg_net` above, but we have not said much about `seg_net`.\n",
    "Here is a flowchart summarizing the final loss computation for `seg_net`,\n",
    "where dotted lines indicate data that is sometimes not available:\n",
    "\n",
    "![segmentation network loss flowchart](notebook_resources/seg_loss_flowchart.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8af785",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training cell\n",
    "# (if already done then you may skip this and uncomment the loading checkpoint cell below)\n",
    "\n",
    "seg_net.to(device)\n",
    "reg_net.to(device)\n",
    "\n",
    "learning_rate_reg = 5e-4\n",
    "optimizer_reg = torch.optim.Adam(reg_net.parameters(), learning_rate_reg)\n",
    "\n",
    "learning_rate_seg = 1e-3\n",
    "optimizer_seg = torch.optim.Adam(seg_net.parameters(), learning_rate_seg)\n",
    "\n",
    "lambda_a = 2.0  # anatomy loss weight\n",
    "lambda_sp = 3.0  # supervised segmentation loss weight\n",
    "\n",
    "# regularization loss weight\n",
    "# This often requires some careful tuning. Here we suggest a value, which unfortunately needs to\n",
    "# depend on image scale. This is because the bending energy loss is not scale-invariant.\n",
    "# 7.5 worked well with the above hyperparameters for images of size 128x128x128.\n",
    "lambda_r = 7.5 * (image_scale / 128)**2\n",
    "\n",
    "max_epochs = 120\n",
    "reg_phase_training_batches_per_epoch = 40\n",
    "seg_phase_training_batches_per_epoch = 5  # Fewer batches needed, because seg_net converges more quickly\n",
    "reg_phase_num_validation_batches_to_use = 40\n",
    "val_interval = 5\n",
    "\n",
    "training_losses_reg = []\n",
    "validation_losses_reg = []\n",
    "training_losses_seg = []\n",
    "validation_losses_seg = []\n",
    "\n",
    "best_seg_validation_loss = float('inf')\n",
    "best_reg_validation_loss = float('inf')\n",
    "\n",
    "for epoch_number in range(max_epochs):\n",
    "\n",
    "    print(f\"Epoch {epoch_number+1}/{max_epochs}:\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    #         reg_net training, with seg_net frozen\n",
    "    # ------------------------------------------------\n",
    "\n",
    "    # Keep computational graph in memory for reg_net, but not for seg_net, and do reg_net.train()\n",
    "    swap_training(reg_net, seg_net)\n",
    "\n",
    "    losses = []\n",
    "    for batch in batch_generator_train_reg(reg_phase_training_batches_per_epoch):\n",
    "        optimizer_reg.zero_grad()\n",
    "        loss_sim, loss_reg, loss_ana = reg_losses(batch)\n",
    "        loss = loss_sim + lambda_r * loss_reg + lambda_a * loss_ana\n",
    "        loss.backward()\n",
    "        optimizer_reg.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    training_loss = np.mean(losses)\n",
    "    print(f\"\\treg training loss: {training_loss}\")\n",
    "    training_losses_reg.append([epoch_number, training_loss])\n",
    "\n",
    "    if epoch_number % val_interval == 0:\n",
    "        reg_net.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in batch_generator_valid_reg(reg_phase_num_validation_batches_to_use):\n",
    "                loss_sim, loss_reg, loss_ana = reg_losses(batch)\n",
    "                loss = loss_sim + lambda_r * loss_reg + lambda_a * loss_ana\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        validation_loss = np.mean(losses)\n",
    "        print(f\"\\treg validation loss: {validation_loss}\")\n",
    "        validation_losses_reg.append([epoch_number, validation_loss])\n",
    "\n",
    "        if validation_loss < best_reg_validation_loss:\n",
    "            best_reg_validation_loss = validation_loss\n",
    "            torch.save(reg_net.state_dict(), 'reg_net_best.pth')\n",
    "\n",
    "    # Free up memory\n",
    "    del loss, loss_sim, loss_reg, loss_ana\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    #         seg_net training, with reg_net frozen\n",
    "    # ------------------------------------------------\n",
    "\n",
    "    # Keep computational graph in memory for seg_net, but not for reg_net, and do seg_net.train()\n",
    "    swap_training(seg_net, reg_net)\n",
    "\n",
    "    losses = []\n",
    "    for batch in batch_generator_train_seg(seg_phase_training_batches_per_epoch):\n",
    "        optimizer_seg.zero_grad()\n",
    "\n",
    "        img12 = batch['img12'].to(device)\n",
    "\n",
    "        displacement_fields = reg_net(img12)\n",
    "        seg1_predicted = seg_net(img12[:, [0], :, :, :]).softmax(dim=1)\n",
    "        seg2_predicted = seg_net(img12[:, [1], :, :, :]).softmax(dim=1)\n",
    "\n",
    "        # Below we compute the following:\n",
    "        # loss_supervised: supervised segmentation loss; compares ground truth seg with predicted seg\n",
    "        # loss_anatomy: anatomy loss; compares warped seg of moving image to seg of target image\n",
    "        # loss_metric: a single supervised seg loss, as a metric to track the progress of training\n",
    "\n",
    "        if 'seg1' in batch.keys() and 'seg2' in batch.keys():\n",
    "            seg1 = monai.networks.one_hot(batch['seg1'].to(device), num_segmentation_classes)\n",
    "            seg2 = monai.networks.one_hot(batch['seg2'].to(device), num_segmentation_classes)\n",
    "            loss_metric = dice_loss2(seg2_predicted, seg2)\n",
    "            loss_supervised = dice_loss2(seg1_predicted, seg1) + loss_metric\n",
    "            # The above supervised loss looks a bit different from the one in the paper\n",
    "            # in that it includes predictions for both images in the current image pair;\n",
    "            # we might as well do this, since we have gone to the trouble of loading\n",
    "            # both segmentations into memory.\n",
    "\n",
    "        elif 'seg1' in batch.keys():  # seg1 available, but no seg2\n",
    "            seg1 = monai.networks.one_hot(batch['seg1'].to(device), num_segmentation_classes)\n",
    "            loss_metric = dice_loss2(seg1_predicted, seg1)\n",
    "            loss_supervised = loss_metric\n",
    "            seg2 = seg2_predicted  # Use this in anatomy loss\n",
    "\n",
    "        else:  # seg2 available, but no seg1\n",
    "            assert('seg2' in batch.keys())\n",
    "            seg2 = monai.networks.one_hot(batch['seg2'].to(device), num_segmentation_classes)\n",
    "            loss_metric = dice_loss2(seg2_predicted, seg2)\n",
    "            loss_supervised = loss_metric\n",
    "            seg1 = seg1_predicted  # Use this in anatomy loss\n",
    "\n",
    "        # seg1 and seg2 should now be in the form of one-hot class probabilities\n",
    "\n",
    "        loss_anatomy = dice_loss2(warp_nearest(seg2, displacement_fields), seg1)\\\n",
    "            if 'seg1' in batch.keys() or 'seg2' in batch.keys()\\\n",
    "            else 0.  # It wouldn't really be 0, but it would not contribute to training seg_net\n",
    "\n",
    "        # (If you want to refactor this code for *joint* training of reg_net and seg_net,\n",
    "        #  then use the definition of anatomy loss given in the function anatomy_loss above,\n",
    "        #  where differentiable warping is used and reg net can be trained with it.)\n",
    "\n",
    "        loss = lambda_a * loss_anatomy + lambda_sp * loss_supervised\n",
    "        loss.backward()\n",
    "        optimizer_seg.step()\n",
    "\n",
    "        losses.append(loss_metric.item())\n",
    "\n",
    "    training_loss = np.mean(losses)\n",
    "    print(f\"\\tseg training loss: {training_loss}\")\n",
    "    training_losses_seg.append([epoch_number, training_loss])\n",
    "\n",
    "    if epoch_number % val_interval == 0:\n",
    "        # The following validation loop would not do anything in the case\n",
    "        # where there is just one segmentation available,\n",
    "        # because data_seg_available_valid would be empty.\n",
    "        seg_net.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader_seg_available_valid:\n",
    "                imgs = batch['img'].to(device)\n",
    "                true_segs = batch['seg'].to(device)\n",
    "                predicted_segs = seg_net(imgs)\n",
    "                loss = dice_loss(predicted_segs, true_segs)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        validation_loss = np.mean(losses)\n",
    "        print(f\"\\tseg validation loss: {validation_loss}\")\n",
    "        validation_losses_seg.append([epoch_number, validation_loss])\n",
    "\n",
    "        if validation_loss < best_seg_validation_loss:\n",
    "            best_seg_validation_loss = validation_loss\n",
    "            torch.save(seg_net.state_dict(), 'seg_net_best.pth')\n",
    "\n",
    "    # Free up memory\n",
    "    del loss, seg1, seg2, displacement_fields, img12, loss_supervised, loss_anatomy, loss_metric,\\\n",
    "        seg1_predicted, seg2_predicted\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n\\nBest reg_net validation loss: {best_reg_validation_loss}\")\n",
    "print(f\"Best seg_net validation loss: {best_seg_validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050dd7b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation losses\n",
    "\n",
    "plot_against_epoch_numbers(training_losses_reg, label=\"training\")\n",
    "plot_against_epoch_numbers(validation_losses_reg, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.ylabel('loss')\n",
    "plt.title('Alternating training: registration loss')\n",
    "plt.savefig('reg_net_losses.png')\n",
    "plt.show()\n",
    "\n",
    "plot_against_epoch_numbers(training_losses_seg, label=\"training\")\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Alternating training: segmentation loss (training)')\n",
    "plt.savefig('seg_net_training_losses.png')\n",
    "plt.show()\n",
    "\n",
    "plot_against_epoch_numbers(validation_losses_seg, label=\"validation\", color='orange')\n",
    "plt.ylabel('validation loss')\n",
    "plt.title('Alternating training: segmentation loss (validation)')\n",
    "plt.savefig('seg_net_validation_losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76998a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; SAVE\n",
    "torch.save(seg_net.state_dict(), 'seg_net.pth')\n",
    "torch.save(reg_net.state_dict(), 'reg_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; LOAD\n",
    "# seg_net.load_state_dict(torch.load('seg_net.pth'))\n",
    "# reg_net.load_state_dict(torch.load('reg_net.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7090315",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "Here we run forward passes for the two networks and we try to get a sense of their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dadc083",
   "metadata": {},
   "source": [
    "### Inference using segmentation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to try out seg net on a random validation pair\n",
    "\n",
    "seg_net.to(device)\n",
    "\n",
    "data_item = random.choice(dataset_seg_available_valid)\n",
    "test_input = data_item['img']\n",
    "test_seg_true = data_item['seg']\n",
    "seg_net.eval()\n",
    "with torch.no_grad():\n",
    "    test_seg_predicted = seg_net(test_input.unsqueeze(0).cuda()).cpu()\n",
    "    loss = dice_loss(test_seg_predicted, test_seg_true.unsqueeze(0)).item()\n",
    "\n",
    "print(\"original image from validation set:\")\n",
    "preview_image(test_input[0], normalize_by=\"slice\", cmap='gray')\n",
    "print(\"ground truth segmentation:\")\n",
    "preview_image(test_seg_true[0])\n",
    "print(\"our predicted segmentation:\")\n",
    "preview_image(torch.argmax(torch.softmax(test_seg_predicted, dim=1), dim=1, keepdim=True)[0, 0])\n",
    "print(f\"dice loss: {loss}\")\n",
    "\n",
    "del test_seg_predicted\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ccc3e",
   "metadata": {},
   "source": [
    "### Inference using registration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801a48e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this cell to try out reg net on a random validation pair\n",
    "\n",
    "reg_net.to(device)\n",
    "reg_net.eval()\n",
    "\n",
    "data_item = take_random_from_subdivided_dataset(dataset_pairs_valid_subdivided)\n",
    "img12 = data_item['img12'].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reg_net_example_output = reg_net(img12)\n",
    "\n",
    "example_warped_image = warp(\n",
    "    img12[:, [1], :, :, :],  # moving image\n",
    "    reg_net_example_output  # warping\n",
    ")\n",
    "\n",
    "# Uncomment to preview displacement field and warped image\n",
    "print(\"moving image:\")\n",
    "preview_image(img12[0, 1, :, :, :].cpu(), normalize_by=\"slice\", cmap='gray')\n",
    "print(\"target image:\")\n",
    "preview_image(img12[0, 0, :, :, :].cpu(), normalize_by=\"slice\", cmap='gray')\n",
    "print(\"warped moving image:\")\n",
    "preview_image(example_warped_image[0, 0].cpu(), normalize_by=\"slice\", cmap='gray')\n",
    "print(\"deformation vector field, projected into viewing planes:\")\n",
    "preview_3D_vector_field(reg_net_example_output.cpu().detach()[0])\n",
    "print(\"deformation applied to a grid, projected into viewing planes:\")\n",
    "preview_3D_deformation(reg_net_example_output.cpu().detach()[0], 2, linewidth=1, color='darkblue')\n",
    "print(\"jacobian determinant:\")\n",
    "det = jacobian_determinant(reg_net_example_output.cpu().detach()[0])\n",
    "preview_image(det, normalize_by='slice', threshold=0)\n",
    "loss = lncc_loss(example_warped_image, img12[:, [0], :, :, :]).item()\n",
    "print(f\"Similarity loss: {loss}\")\n",
    "print(f\"number of folds: {(det<=0).sum()}\")\n",
    "\n",
    "del reg_net_example_output, img12, example_warped_image\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca6e7a",
   "metadata": {},
   "source": [
    "That last image, the Jacobian determinant field, is a useful way to visualize the effect of the deformation generated by `reg_net`. This is a scalar field that tells us, at each voxel, the scale factor by which volumes are scaled near that voxel. Where it's negative, it's telling us that the deformation map locally reverses orientation at that location. We refer to voxels where the Jacobian determinant is negative as \"folds.\" Folds are shown in red in the preview above.\n",
    "\n",
    "<!-- This isn't perfect, of course. We don't really have a map from a subset of $\\mathbb{R}^3$ to $\\mathbb{R}^3$, but a discretely sampled version of one. We are not taking derivatives-- we are taking discrete differences. So some folds here and there might not really indicate anything bad... hmm not sure about this... commenting out for now -->\n",
    "\n",
    "The number of folds that occur while registering a pair of images is a nice way to think about how \"bad\" the deformation is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753507b",
   "metadata": {},
   "source": [
    "### Count folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4dd70e",
   "metadata": {},
   "source": [
    "Here we evaluate the regularity of `reg_net`\n",
    "by sampling some image pairs from the validation set and counting folds.\n",
    "\n",
    "If the results here look bad, then it makes sense to try training with an increased `lambda_r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches_for_histogram = 200  # sample this many batches\n",
    "\n",
    "fold_counts = []  # Each element is the number of folds that occur in the registration of some image pair\n",
    "negative_det_values = []  # Each element is the jacobian determinant at some fold for some image pair\n",
    "reg_net.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in batch_generator_valid_reg(num_batches_for_histogram):\n",
    "        img12 = batch['img12'].to(device)\n",
    "        displacement_fields = reg_net(img12)\n",
    "        for displacement_field in displacement_fields:\n",
    "            det = jacobian_determinant(displacement_field.cpu())\n",
    "            num_folds = (det <= 0).sum()\n",
    "            fold_counts.append(num_folds)\n",
    "            if len(negative_det_values) < 1e6:  # Limit how many of these values we store\n",
    "                negative_det_values += det[det <= 0].tolist()\n",
    "\n",
    "del img12, displacement_fields\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "plt.hist(fold_counts)\n",
    "plt.title(\"Number of folds\")\n",
    "plt.savefig('histogram_num_folds.png')\n",
    "plt.show()\n",
    "print(f\"Mean fold count: {np.mean(fold_counts)} folds, out of {det.size} locations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b11d6",
   "metadata": {},
   "source": [
    "We may also want to look at how \"bad\" the folds get. If the negative jacobian determinants are almost always close to zero, then things aren't looking too bad. If they consistently take larger negative values, then this indicates a serious issue with the deformations produced by `reg_net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(negative_det_values)\n",
    "\n",
    "# View counts logarithmically to see details for the worse jacobians, where counts are usually smaller\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.title(\"Negative Jacobian values\")\n",
    "plt.savefig('histogram_neg_jac.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3392f1b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b58db1",
   "metadata": {},
   "source": [
    "_This work was supported by the National Institutes of Health\n",
    "under Award Number R42MH118845. The content is solely the responsibility of the authors and does not\n",
    "necessarily represent the official views of the National Institutes of Health._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
