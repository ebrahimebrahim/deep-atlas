{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49062f69",
   "metadata": {},
   "source": [
    "# Loading OASIS images\n",
    "\n",
    "See [fact sheet here](https://www.oasis-brains.org/files/oasis_cross-sectional_facts.pdf) for info on OASIS-1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a99ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path\n",
    "oasis_dir = \"/home/ebrahim/data/OASIS-1/\"\n",
    "image_paths = glob.glob(os.path.join(oasis_dir,'*/PROCESSED/MPRAGE/T88_111/OAS1_*_MR*_mpr_n*_anon_111_t88_masked_gfc.img'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819652b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's load one image and look at it, just to establish how we want to load these images\n",
    "import random\n",
    "import itk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supress the many warnings related to depracation of the Analyze file format\n",
    "itk.ProcessObject.SetGlobalWarningDisplay(False) \n",
    "\n",
    "image_path = random.choice(image_paths)\n",
    "itk_image = itk.imread(image_path)\n",
    "image_array = itk.array_from_image(itk_image)\n",
    "\n",
    "def preview_image(image_array):\n",
    "    x,y,z = np.array(image_array.shape)//2 # half-way slices\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(1,3,1); plt.axis('off')\n",
    "    plt.imshow(image_array[x,:,:], origin = 'lower')\n",
    "    plt.subplot(1,3,2); plt.axis('off')\n",
    "    plt.imshow(image_array[:,y,:], origin = 'lower')\n",
    "    plt.subplot(1,3,3); plt.axis('off')\n",
    "    plt.imshow(image_array[:,:,z], origin = 'lower')\n",
    "    plt.show()\n",
    "\n",
    "preview_image(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10033835",
   "metadata": {},
   "source": [
    "Actually, MONAI's [image loading transform](https://docs.monai.io/en/latest/transforms.html#loadimage) can already use itk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a509909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "\n",
    "image_path = random.choice(image_paths)\n",
    "img_itk_array = monai.transforms.LoadImage(image_only=True)(image_path)\n",
    "preview_image(img_itk_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = monai.data.Dataset(\n",
    "    data = image_paths,\n",
    "    transform = monai.transforms.Compose(\n",
    "        transforms = [\n",
    "            monai.transforms.LoadImage(image_only=True)\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0000df",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_image(dataset[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144a80f5",
   "metadata": {},
   "source": [
    "I think affine registration is already done for the these images, so we don't need to worry about including an affine part in the registration network. Let's look into loading a segmentation label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a76c4",
   "metadata": {},
   "source": [
    "Hmm, how should the data set work considering that we want to consider a general situation where some images have seg labels and some don't? \n",
    "\n",
    "One option: it could always return an (image,label) pair, but return `None` for the label when the segmentation isn't available. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ae191",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_paths = glob.glob(os.path.join(oasis_dir,'*/FSL_SEG/OAS1_*_MR*_mpr_n*_anon_111_t88_masked_gfc_fseg.img'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d471ed",
   "metadata": {},
   "source": [
    "examine the segmentations provided by OASIS... what are they and how are they obtained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa461c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_id = lambda path : os.path.basename(path).strip('OAS1_')[:4]\n",
    "\n",
    "seg_ids = list(map(path_to_id, segmentation_paths))\n",
    "img_ids = map(path_to_id, segmentation_paths)\n",
    "image_label_pairs = []\n",
    "for img_index,img_id in enumerate(img_ids):\n",
    "    seg_index = seg_ids.index(img_id) if (img_id in seg_ids) else None\n",
    "    seg_path = segmentation_paths[seg_index] if (seg_index is not None) else None\n",
    "    img_path = image_paths[img_index]\n",
    "    image_label_pairs.append(\n",
    "        {\n",
    "            'img' : img_path,\n",
    "             'seg' : seg_path\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71cdbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example_pair = monai.transforms.LoadImaged(keys=['img','seg'],image_only=True)(random.choice(image_label_pairs))\n",
    "preview_image(example_pair['img'])\n",
    "preview_image(example_pair['seg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc470eed",
   "metadata": {},
   "source": [
    "So we have a segmentaton for each image. Actually, these segmentations are done by an algorithm:\n",
    "\n",
    "*Segmentation of brain MR images through a hidden Markov random field model and the expectation maximization algorithm*\\\n",
    "2001 Zhang, Brady, Smith\\\n",
    "[link](https://pubmed.ncbi.nlm.nih.gov/11293691/)\n",
    "\n",
    "The situation in which we'd want to apply DeepAtlas is one where we have just a handful of carefully created manual segmengations. Let's simulate this situation by pretending that some of the segmentation labels don't exist, and ignoring the fact that the segmentations we have come from an algorithm.\n",
    "\n",
    "Next: Repeat the cell where you create `image_label_pairs`, but this time replace `seg_ids` with a randomly selected small-ish subset of itself. Then see if you can make `monai.transforms.LoadImaged` eat the `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pretend that only a few segmentations are available.\n",
    "# For all but a few items in image_label_pairs, we set the 'seg' to None.\n",
    "\n",
    "num_segs_to_select = 17\n",
    "random.shuffle(image_label_pairs)\n",
    "for image_label_dict in image_label_pairs[:(len(image_label_pairs)-num_segs_to_select)]:\n",
    "    image_label_dict['seg'] = None\n",
    "random.shuffle(image_label_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276e21f",
   "metadata": {},
   "source": [
    "Now this could be viewed as sort of a generic starting point for data to be used with DeepAtlas.\n",
    "We start here with `image_label_pairs` being a list of dictionaries with `'img'` and `'seg'` as keys, and with the `'seg'` key often being `None`.\n",
    "\n",
    "# Creating datasets from our data with partial labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c43dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_seg_available = monai.data.Dataset(\n",
    "    data = list(filter(lambda d : d['seg'] is not None ,image_label_pairs)),\n",
    "    transform = monai.transforms.Compose(\n",
    "        transforms = [\n",
    "            monai.transforms.LoadImageD(keys=['img','seg'], image_only=True),\n",
    "            monai.transforms.ToTensorD(keys=['img','seg']),\n",
    "            monai.transforms.AddChannelD(keys=['img','seg'])\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "dataset_seg_unavailable = monai.data.Dataset(\n",
    "    data = list(filter(lambda d : d['seg'] is None ,image_label_pairs)),\n",
    "    transform = monai.transforms.Compose(\n",
    "        transforms = [\n",
    "            monai.transforms.LoadImageD(keys=['img'], image_only=True),\n",
    "            monai.transforms.ToTensorD(keys=['img']),\n",
    "            monai.transforms.AddChannelD(keys=['img'])\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee6da5",
   "metadata": {},
   "source": [
    "Next: Create registration and segmentation models and losses as described in the paper and see that you can send the data through.\n",
    "\n",
    "# Segmentation network and dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fcdd44",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# It's not the same as the paper, since the paper does maxpooling for it's seg net,\n",
    "# while this UNet does convolution with stride>1 for the downsampling steps.\n",
    "# Also the paper uses leaky relu activation and this uses prelu.\n",
    "# Also the paper does batch norm while this does instance norm\n",
    "\n",
    "seg_net = monai.networks.nets.UNet(\n",
    "    3, # spatial dims\n",
    "    1, # input channels\n",
    "    4, # output channels (4 segmentation classes)\n",
    "    (8,16,16,32,32,64,64), # channel sequence\n",
    "    (1,2,1,2,1,2) # convolutional strides\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0f0a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try out a forward pass\n",
    "\n",
    "# It's pretty fortunate that the OASIS images have all their spatial dimensions being multiples of 8,\n",
    "# cosnidering that we downsample by a factor of 2 three time in the UNet. Very clean.\n",
    "\n",
    "seg_net_example_output = seg_net(dataset_seg_available[0]['img'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cba14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.unique(dataset_seg_available[0]['seg'])) # segmentation classes\n",
    "print(dataset_seg_available[0]['seg'].unsqueeze(0).shape) # shape of ground truth label\n",
    "print(seg_net_example_output.shape) # shape of seg net output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ca8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example: the segmentation label and the prediction for the voxel at (128,128,128)\n",
    "\n",
    "print(dataset_seg_available[0]['seg'][:,128,128,128])\n",
    "print(torch.softmax(seg_net_example_output, dim=1)[0,:,128,128,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56244df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = monai.losses.DiceLoss(\n",
    "    include_background=True, # Include background in the multiclass DICE loss: background, CSF, grey, white\n",
    "    to_onehot_y=True, # Our seg labels are single channel images indicating class index, rather than one-hot\n",
    "    softmax=True, # Note that our segmentation network is missing the softmax at the end\n",
    "    reduction=\"mean\" # Follows the paper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try out the dice loss on an example.\n",
    "dice_loss(\n",
    "    seg_net_example_output, # Prediction from seg_net\n",
    "    dataset_seg_available[0]['seg'].unsqueeze(0) # Ground truth label\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b82eecd",
   "metadata": {},
   "source": [
    "# Registration network and NCC loss\n",
    "\n",
    "Do this next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb589363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not exactly identical to the registration network in the paper, but similar\n",
    "# The main difference is that this one doesn't get to the very middle step labeled 1/16\n",
    "# It seems that MONAI UNet design will not stick one block in the middle like we see in fig 3 of the paper\n",
    "\n",
    "reg_net = monai.networks.nets.UNet(\n",
    "    3, # spatial dims\n",
    "    2, # input channels (one for fixed image and one for moving image)\n",
    "    3, # output channels (to represent 3D displacement vector field)\n",
    "    (16,32,32,32,32), # channel sequence\n",
    "    (1,2,2,2,2) # convolutional strides\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out a forward pass\n",
    "\n",
    "fixed_image_example = dataset_seg_unavailable[0]['img'].unsqueeze(0)\n",
    "moving_image_example = dataset_seg_unavailable[1]['img'].unsqueeze(0)\n",
    "\n",
    "#Concatenate two images along channel dimension, to serve as a \"fixed\" and a \"moving\" image\n",
    "reg_net_example_input = torch.cat([fixed_image_example, moving_image_example],1)\n",
    "print(reg_net_example_input.shape)\n",
    "\n",
    "reg_net_example_output = reg_net(reg_net_example_input)\n",
    "print(reg_net_example_output.shape)\n",
    "# The output of the reg_net is assumed to be a displacement vector field\n",
    "# (so e.g. a zero output would be the identity warping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local normalized cross-correlation loss.\n",
    "# The paper does a global NCC, but MONAI provides local and it's easy to use.\n",
    "# Another difference from the paper: instead of returning 1-LNCC this will return -LNCC\n",
    "\n",
    "lncc_loss = monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
    "    ndim=3, # this keyword argument will soon be renamed to spatial_dims\n",
    "    kernel_size=3,\n",
    "    kernel_type='rectangular',\n",
    "    reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcca5e",
   "metadata": {},
   "source": [
    "The output of `reg_net` is a displacement field.\n",
    "In order to compute the resulting image similarity loss, we must warp the moving image using the displacement field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = monai.networks.blocks.Warp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f859629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an identity image mapping to test out warp and make sure it works the way I expect\n",
    "x_dim, y_dim, z_dim = fixed_image_example[0,0].shape\n",
    "x_proj = torch.tensor(range(x_dim))\n",
    "x_proj = x_proj.repeat((z_dim,y_dim,1))\n",
    "x_proj = x_proj.permute((2,1,0))\n",
    "y_proj = torch.tensor(range(y_dim))\n",
    "y_proj = y_proj.repeat((x_dim,z_dim,1))\n",
    "y_proj = y_proj.permute((0,2,1))\n",
    "z_proj = torch.tensor(range(z_dim))\n",
    "z_proj = z_proj.repeat((x_dim,y_dim,1))\n",
    "identity_map = torch.stack([x_proj,y_proj,z_proj])\n",
    "identity_map = identity_map.float()\n",
    "print(identity_map.shape)\n",
    "identity_map[:,33,44,55] # Should be [33,44,55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_image(moving_image_example[0,0])\n",
    "preview_image(warp(moving_image_example,identity_map.unsqueeze(0))[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b5d523",
   "metadata": {},
   "source": [
    "Hmm I see, it seems that warp is expecting a displacement vector field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_displacement = torch.zeros((3,x_dim,y_dim,z_dim))\n",
    "preview_image(warp(moving_image_example,zero_displacement.unsqueeze(0))[0,0])\n",
    "# Ah okay, warp is expected a displacement vector field, not an image mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9bedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test out warping using the example reg_net output from above\n",
    "\n",
    "example_warped_image = warp(moving_image_example, reg_net_example_output)  \n",
    "print(example_warped_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0431f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's test our image similarity loss function\n",
    "\n",
    "lncc_loss(\n",
    "    example_warped_image, # prediciton\n",
    "    fixed_image_example # target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5c237",
   "metadata": {},
   "source": [
    "# Regularization: bending energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb69f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "bending_loss = monai.losses.BendingEnergyLoss()\n",
    "\n",
    "print(bending_loss(zero_displacement.unsqueeze(0)))\n",
    "print(bending_loss(reg_net_example_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74344fa",
   "metadata": {},
   "source": [
    "# Preparing datasets more carefully\n",
    "\n",
    "We are going to start by pre-training the segmentation network on the few segmentations that we have. Then we will jointly train the segmentation and the registration networks using all available image pairs. In the training loop if both images are unlabeled then we skip the learning step for the segmentation network, since there is no source of truth for it. \n",
    "\n",
    "During the segmentation network pretraining, we just want `dataset_seg_available` as defined above.\n",
    "\n",
    "During the joint training, there are two important cases to care about:\n",
    "- Image pairs where at least one of them has a segmentation label.\n",
    "- Image pairs where neither of them have a segmentation label.\n",
    "\n",
    "Since the second one doesn't allow for `seg_net` learning, we want these two to be separate so that we can have direct control over the relative rates of training `reg_net` vs `seg_net` (rather than let it be controlled by the proportion of images that have segmentation labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5aaf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create image pair datasets of the two types described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad752ee",
   "metadata": {},
   "source": [
    "# Pre-training the segmentation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fec78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remember how we built image_label_pairs above? Let's redo the datasets from there\n",
    "\n",
    "image_label_pairs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5cdc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label_pairs_seg_available  =  list(filter(lambda d : d['seg'] is not None ,image_label_pairs))\n",
    "image_label_pairs_seg_unavailable = list(filter(lambda d : d['seg'] is None ,image_label_pairs))\n",
    "\n",
    "image_label_pairs_seg_available_train, image_label_pairs_seg_available_valid = \\\n",
    "    monai.data.utils.partition_dataset(image_label_pairs_seg_available, ratios=(8,2))\n",
    "\n",
    "transform_seg_available = monai.transforms.Compose(\n",
    "    transforms = [\n",
    "        monai.transforms.LoadImageD(keys=['img','seg'], image_only=True),\n",
    "        monai.transforms.ToTensorD(keys=['img','seg']),\n",
    "        monai.transforms.AddChannelD(keys=['img','seg'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_seg_available_train = monai.data.CacheDataset(\n",
    "    data = image_label_pairs_seg_available_train,\n",
    "    transform = transform_seg_available,\n",
    ")\n",
    "\n",
    "dataset_seg_available_valid = monai.data.CacheDataset(\n",
    "    data = image_label_pairs_seg_available_valid,\n",
    "    transform = transform_seg_available,\n",
    ")\n",
    "\n",
    "dataloader_seg_available_train = monai.data.DataLoader(\n",
    "    dataset_seg_available_train,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_seg_available_valid = monai.data.DataLoader(\n",
    "    dataset_seg_available_valid,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10339df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_net.to('cuda')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(seg_net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f4f31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_epochs = 30\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch_number in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    seg_net.train()\n",
    "    losses = []\n",
    "    for batch in dataloader_seg_available_train:\n",
    "        imgs = batch['img'].to('cuda')\n",
    "        true_segs = batch['seg'].to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_segs = seg_net(imgs)\n",
    "        loss = dice_loss(predicted_segs, true_segs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    training_loss = np.mean(losses)\n",
    "    \n",
    "    seg_net.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader_seg_available_valid:\n",
    "            imgs = batch['img'].to('cuda')\n",
    "            true_segs = batch['seg'].to('cuda')\n",
    "            predicted_segs = seg_net(imgs)\n",
    "            loss = dice_loss(predicted_segs, true_segs)\n",
    "            losses.append(loss.item())\n",
    "        validation_loss = np.mean(losses)\n",
    "        \n",
    "    print(f\"training loss: {training_loss}, validation loss: {validation_loss}\")\n",
    "    preview_image(torch.argmax(torch.softmax(seg_net(dataset_seg_available_valid[0]['img'].unsqueeze(0).cuda()),dim=1),dim=1, keepdim=True)[0,0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; SAVE\n",
    "torch.save(seg_net.state_dict(),'seg_net_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c695ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; LOAD\n",
    "seg_net.load_state_dict(torch.load('seg_net_pretrained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685aabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get seg_net out of graphics memory to make room for other things coming up next\n",
    "seg_net.to('cpu');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e36fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's try out our pretrained seg net on one of the validation images\n",
    "\n",
    "test_input = dataset_seg_available_valid[0]['img'].unsqueeze(0)\n",
    "test_seg_predicted = seg_net(test_input)\n",
    "\n",
    "# Original image from validation set\n",
    "preview_image(dataset_seg_available_valid[0]['img'][0])\n",
    "# Ground truth segmentation\n",
    "preview_image(dataset_seg_available_valid[0]['seg'][0])\n",
    "# Our predicted segmentation\n",
    "seg_net.eval()\n",
    "preview_image(torch.argmax(torch.softmax(seg_net(dataset_seg_available_valid[0]['img'].unsqueeze(0)),dim=1),dim=1, keepdim=True)[0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
