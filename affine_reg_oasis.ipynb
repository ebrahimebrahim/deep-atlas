{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717aa381",
   "metadata": {},
   "source": [
    "Experimenting with affine only registration using deep learning, applied to the OASIS-1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import torch\n",
    "import itk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "import os.path\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "from utils import (\n",
    "    preview_image, preview_3D_vector_field, preview_3D_deformation,\n",
    "    jacobian_determinant, plot_against_epoch_numbers\n",
    ")\n",
    "\n",
    "monai.config.print_config()\n",
    "\n",
    "# Set deterministic training for reproducibility\n",
    "monai.utils.set_determinism(seed=2938649572)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbeaf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if not root_dir:\n",
    "    raise Exception(\"Need to set MONAI_DATA_DIRECTORY env var\")\n",
    "data_dir = os.path.join(root_dir, \"OASIS-1\")\n",
    "print(f\"Root directory: {root_dir}\")\n",
    "print(f\"Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_expression = \"PROCESSED/MPRAGE/T88_111/OAS1_*_MR*_mpr_n*_anon_111_t88_masked_gfc.img\"\n",
    "\n",
    "# Expect either of two reasonable ways of organizing extracted data:\n",
    "# 1) <data_dir>/disc1/OAS1_0031_MR1/...\n",
    "# 2) <data_dir>/OAS1_0031_MR1/...\n",
    "image_paths = glob.glob(os.path.join(data_dir, '*', image_path_expression))\n",
    "image_paths += glob.glob(os.path.join(data_dir, '*/*', image_path_expression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19408a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_train, image_paths_valid = \\\n",
    "    monai.data.utils.partition_dataset(image_paths, ratios=(8, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs_train = [{'fixed':img0, 'moving':img1} for img0 in image_paths_train for img1 in image_paths_train]\n",
    "data_pairs_valid = [{'fixed':img0, 'moving':img1} for img0 in image_paths_valid for img1 in image_paths_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 64\n",
    "resize = (S,S,S)\n",
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13828337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the overall scale of affine transform\n",
    "a=0.3\n",
    "\n",
    "keys=['fixed', 'moving']\n",
    "\n",
    "rand_affine_params = {\n",
    "    'prob':1.,\n",
    "    'mode': 'bilinear',\n",
    "    'padding_mode': 'zeros',\n",
    "    'spatial_size':resize,\n",
    "    'cache_grid':True,\n",
    "    'rotate_range': (a*np.pi/2,)*3,\n",
    "    'shear_range': (0,)*6, # no shearing\n",
    "    'translate_range': (a*S/16,)*3,\n",
    "    'scale_range': (a*0.2,)*3,\n",
    "}\n",
    "\n",
    "transform = monai.transforms.Compose([\n",
    "    monai.transforms.LoadImageD(reader='itkreader', keys=keys),\n",
    "    monai.transforms.TransposeD(keys=keys, indices=(2,1,0)),\n",
    "    monai.transforms.AddChannelD(keys=keys),\n",
    "    monai.transforms.ToTensorD(keys=keys),\n",
    "    monai.transforms.ResizeD(spatial_size=resize, keys=keys),\n",
    "    monai.transforms.ToDeviceD(keys=keys, device=device),\n",
    "    monai.transforms.RandAffineD(keys='fixed', **rand_affine_params),\n",
    "    monai.transforms.RandAffineD(keys='moving', **rand_affine_params),\n",
    "    monai.transforms.ConcatItemsD(keys=keys, name='fm', dim=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9552062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress the many warnings related to depracation of the Analyze file format\n",
    "# (without this, we would see warnings when the LoadImage transform calls itk to load Analyze files)\n",
    "itk.ProcessObject.SetGlobalWarningDisplay(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will redefine these as CacheDatasets later\n",
    "dataset_pairs_train = monai.data.Dataset(data = data_pairs_train, transform=transform)\n",
    "dataset_pairs_valid = monai.data.Dataset(data = data_pairs_valid, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine\n",
    "\n",
    "d = random.choice(dataset_pairs_train)\n",
    "preview_image(d['fm'][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally stolen from:\n",
    "# https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html\n",
    "\n",
    "class AffineNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AffineNet, self).__init__()\n",
    "\n",
    "        self.dropout_p = 0.2\n",
    "        \n",
    "        # Spatial transformer localization-network\n",
    "        self.loc_C1 = 8\n",
    "        self.loc_C2 = 16\n",
    "        self.loc_C3 = 32\n",
    "        self.localization = torch.nn.Sequential( # say input is shape (B,2,S,S,S)\n",
    "            torch.nn.Conv3d(2, self.loc_C1, kernel_size=5), # -4\n",
    "            torch.nn.MaxPool3d(2, stride=2), # /2\n",
    "            torch.nn.BatchNorm3d(self.loc_C1), torch.nn.Dropout(p=self.dropout_p),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.Conv3d(self.loc_C1, self.loc_C2, kernel_size=3), # -2\n",
    "            torch.nn.MaxPool3d(2, stride=2), # /2\n",
    "            torch.nn.BatchNorm3d(self.loc_C2), torch.nn.Dropout(p=self.dropout_p),\n",
    "            torch.nn.PReLU(), # output should be (B,2,S',S',S') where S' = ((S-4)/2 - 2)/2\n",
    "            torch.nn.Conv3d(self.loc_C2, self.loc_C3, kernel_size=3), # -2\n",
    "#             torch.nn.MaxPool3d(2, stride=2), # /2\n",
    "            torch.nn.BatchNorm3d(self.loc_C3), torch.nn.Dropout(p=self.dropout_p),\n",
    "            torch.nn.PReLU(),\n",
    "        )\n",
    "        \n",
    "        self.S0 = ((S-4)//2 - 2)//2 - 2 # Spatial size expected after self.localization\n",
    "\n",
    "        # Regressor for the affine transform parameters (a 4 by 3 matrix)\n",
    "        self.fc_loc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.loc_C3 * self.S0**3, 4*3),\n",
    "#             torch.nn.PReLU(),\n",
    "#             torch.nn.Linear(32, 4 * 3)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[-1].weight.data.zero_()\n",
    "        self.fc_loc[-1].bias.data.copy_(torch.tensor([1,0,0,0, 0,1,0,0, 0,0,1,0], dtype=torch.float))\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        assert(len(set(xs.shape[2:]))==1 and xs.shape[2]==self.S0)\n",
    "        xs = xs.view(-1, self.loc_C3 * self.S0**3 )\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 3, 4)\n",
    "\n",
    "        grid = torch.nn.functional.affine_grid(theta, x.size(), align_corners=False)\n",
    "        x = torch.nn.functional.grid_sample(x[:,[1]], grid, align_corners=False)\n",
    "\n",
    "        return theta, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_affine_net():\n",
    "    return AffineNet()\n",
    "\n",
    "model = make_affine_net().to(device)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"model has {num_params} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01698b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine\n",
    "\n",
    "d=random.choice(dataset_pairs_train)\n",
    "fixed = d['fixed'].unsqueeze(0)\n",
    "moving = d['moving'].unsqueeze(0)\n",
    "\n",
    "print(\"fixed:\"); preview_image(fixed[0,0].cpu())\n",
    "print(\"moving:\"); preview_image(moving[0,0].cpu())\n",
    "\n",
    "theta, warped = model(d['fm'].unsqueeze(0))\n",
    "print(\"warped:\"); preview_image(warped[0,0].detach().cpu())\n",
    "print(\"theta:\"); print(theta.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ec7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perfect_transform_monai_coords(d):\n",
    "    phi1 = [t for t in d['fixed_transforms'] if t['class']=='RandAffined'][0]['extra_info']['affine']\n",
    "    phi2 = [t for t in d['moving_transforms'] if t['class']=='RandAffined'][0]['extra_info']['affine']\n",
    "    return torch.linalg.solve(phi2, phi1)\n",
    " \n",
    "N = torch.tensor([[0, 0, 2/(S-1), 0], [0,2/(S-1),0,0], [2/(S-1),0,0,0], [0,0,0,1]], dtype=torch.float32).to(device)\n",
    "def get_perfect_transform_torch_coords(d):\n",
    "    theta = get_perfect_transform_monai_coords(d).to(device)\n",
    "    # We need to convert monai coords, which are based on grid values in [-(size-1)/2, (size-1)/2]\n",
    "    # to torch coords, which have different index ordering and are based on grid values in [-1,1]\n",
    "    return torch.matmul(N, torch.matmul(theta, torch.linalg.inv(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ebfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncc_loss = monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
    "    spatial_dims=3,\n",
    "    kernel_size=7,\n",
    "    kernel_type='rectangular',\n",
    "    reduction=\"mean\",\n",
    "    smooth_nr = 1e-6, # Make sure to make smooth_nr quite a bit smaller than smooth_dr!\n",
    "    smooth_dr = 1e-3, # Don't make this too small, for the sake of numerical stability\n",
    ")\n",
    "mse_loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483644ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out by eyeballing and checking similarity losses\n",
    "d=random.choice(dataset_pairs_train)\n",
    "print(\"fixed:\"); preview_image(d['fixed'][0].cpu())\n",
    "print(\"moving:\"); preview_image(d['moving'][0].cpu())\n",
    "\n",
    "theta_monai = get_perfect_transform_monai_coords(d)\n",
    "theta_torch = get_perfect_transform_torch_coords(d).unsqueeze(0).type(torch.float32).to(device)\n",
    "theta_torch = theta_torch[:,:3]\n",
    "\n",
    "\n",
    "perfectly_transformed_moving =\\\n",
    "    monai.transforms.Affine(affine=theta_monai,padding_mode='zeros')(d['moving'])[0]\n",
    "print('moving warped by perfect transform:'); preview_image(perfectly_transformed_moving[0].cpu())\n",
    "print(\"perfect transform in monai coords:\"); print(theta_monai)\n",
    "l1 = mse_loss(perfectly_transformed_moving.unsqueeze(0), fixed).item()\n",
    "l2 = lncc_loss(perfectly_transformed_moving.unsqueeze(0), fixed).item()\n",
    "print(\"losses:\",l1,l2)\n",
    "\n",
    "x=d['fm'].unsqueeze(0)\n",
    "print(\"perfect transform again, torch coords:\"); print(theta_torch)\n",
    "grid = torch.nn.functional.affine_grid(theta_torch, x[:,[1]].size(), align_corners=False)\n",
    "x = torch.nn.functional.grid_sample(x[:,[1]], grid, align_corners=False)\n",
    "print('moving warped by perfect transform, using torch coords this time:'); preview_image(x[0,0].cpu())\n",
    "l1 = mse_loss(x, fixed).item()\n",
    "l2 = lncc_loss(x, fixed).item()\n",
    "print(\"losses:\",l1,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pairs_train = monai.data.CacheDataset(data = data_pairs_train, transform=transform, cache_num=256)\n",
    "# dataset_pairs_valid = monai.data.CacheDataset(data = data_pairs_valid, transform=transform, cache_num=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a404cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_pairs_train = monai.data.DataLoader(\n",
    "    dataset_pairs_train,\n",
    "    batch_size=4,\n",
    "    num_workers=0,\n",
    "    shuffle=True\n",
    ")\n",
    "model = make_affine_net().to(device) # Reinitialize weights (convenient to do that in this cell)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "losses_to_plot = []\n",
    "print_loss_every = 1\n",
    "batches_per_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32925eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42018f7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    batch_number = 0\n",
    "    for batch in dataloader_pairs_train:\n",
    "        if batch_number >= batches_per_epoch: break\n",
    "        batch_number += 1\n",
    "        opt.zero_grad()\n",
    "\n",
    "        theta, warped = model(batch['fm'])\n",
    "#         grid = torch.nn.functional.affine_grid(theta, moving.size(), align_corners=False)\n",
    "#         warped = torch.nn.functional.grid_sample(moving, grid, align_corners=False)\n",
    "        \n",
    "#         loss = mse_loss(warped , fixed) + lambda_kpvar * (fixed_trcov.mean() + moving_trcov.mean())\n",
    "\n",
    "        # Drop the last row from these transforms-- it is always 0,0,0,1\n",
    "        # (This leaves us with 3x4 matrices, so it matches theta's dimensions)\n",
    "        perfect_theta = torch.stack([get_perfect_transform_torch_coords(d) for d in monai.data.decollate_batch(batch)])[:,:3].to(device)\n",
    "\n",
    "        loss = ((theta-perfect_theta)**2).sum()\n",
    "         \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    if epoch % print_loss_every == 0 :\n",
    "        mean_loss = np.mean(losses)\n",
    "        print(f'{epoch}: {mean_loss}')\n",
    "        losses_to_plot.append(mean_loss)\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b65be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '002'\n",
    "save_path = f'model_{run_id}.pth'\n",
    "if os.path.exists(save_path):\n",
    "    raise Exception(\"change run_id before saving\")\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '001'\n",
    "load_path = f'model_{run_id}.pth'\n",
    "if not os.path.exists(save_path):\n",
    "    raise Exception(f\"model {save_path} not found\")\n",
    "model.load_state_dict(torch.load(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comaprison with ants\n",
    "\n",
    "import ants,time\n",
    "\n",
    "model.eval()\n",
    "d=random.choice(dataset_pairs_train)\n",
    "fixed = d['fixed'].unsqueeze(0)\n",
    "moving = d['moving'].unsqueeze(0)        \n",
    "start_time = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    theta, warped = model(d['fm'].unsqueeze(0))\n",
    "my_time = time.perf_counter() - start_time\n",
    "print(theta.detach().cpu())\n",
    "print(\"warped moving image:\")\n",
    "preview_image(warped[0,0].cpu())\n",
    "print(\"target image:\")\n",
    "preview_image(fixed[0,0].cpu())\n",
    "print(\"original moving image:\")\n",
    "preview_image(moving[0,0].cpu())\n",
    "loss = mse_loss(warped,fixed)\n",
    "print(\"my mse loss:\",loss.item())\n",
    "\n",
    "print(\"ants warped image:\")\n",
    "ants_fixed = ants.from_numpy(fixed.cpu().numpy()[0,0])\n",
    "ants_moving = ants.from_numpy(moving.cpu().numpy()[0,0])\n",
    "start_time = time.perf_counter()\n",
    "ants_reg = ants.registration(ants_fixed, ants_moving, type_of_transform='Affine')\n",
    "ants_time = time.perf_counter() - start_time\n",
    "preview_image(ants_reg['warpedmovout'].numpy())\n",
    "loss = mse_loss(torch.tensor(ants_reg['warpedmovout'].numpy()).unsqueeze(0).unsqueeze(0), fixed.cpu())\n",
    "print(\"ants mse loss:\",loss.item())\n",
    "\n",
    "print(f\"My time: {my_time}, ants time: {ants_time}\")\n",
    "\n",
    "print(\"For reference, the known perfect transform:\")\n",
    "perfect_theta = get_perfect_transform_torch_coords(d).unsqueeze(0).type(torch.float32).to(device)\n",
    "perfect_theta = perfect_theta[:,:3]\n",
    "print(perfect_theta)\n",
    "x=d['fm'].unsqueeze(0)\n",
    "grid = torch.nn.functional.affine_grid(perfect_theta, x[:,[1]].size(), align_corners=False)\n",
    "x = torch.nn.functional.grid_sample(x[:,[1]], grid, align_corners=False)\n",
    "print('moving warped by perfect transform:'); preview_image(x[0,0].cpu())\n",
    "loss = mse_loss(x, fixed)\n",
    "print('perfect transform mse loss:', loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
