{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d697b614",
   "metadata": {},
   "source": [
    "# DeepAtlas\n",
    "\n",
    "https://arxiv.org/abs/1904.08465\n",
    "\n",
    "`exploration3`: refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import torch\n",
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad668fbc",
   "metadata": {},
   "source": [
    "# Loading OASIS images\n",
    "\n",
    "See [fact sheet here](https://www.oasis-brains.org/files/oasis_cross-sectional_facts.pdf) for info on OASIS-1 dataset. Get [data here](https://www.oasis-brains.org/#data).\n",
    "\n",
    "## Filepaths\n",
    "\n",
    "We already have a segmentaton for each image. These segmentations were originally done by an algorithm:\n",
    "\n",
    "*Segmentation of brain MR images through a hidden Markov random field model and the expectation maximization algorithm*\\\n",
    "2001 Zhang, Brady, Smith\\\n",
    "[link](https://pubmed.ncbi.nlm.nih.gov/11293691/)\n",
    "\n",
    "The situation in which we'd want to apply DeepAtlas is one where we have just a handful of carefully created manual segmengations. We will simulate this situation by pretending that some of the segmentation labels don't exist, and ignoring the fact that the segmentations we have come from an algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada63a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "oasis_dir = \"/data/ebrahim-data/OASIS-1/\"\n",
    "image_paths = glob.glob(os.path.join(oasis_dir,'*/PROCESSED/MPRAGE/T88_111/OAS1_*_MR*_mpr_n*_anon_111_t88_masked_gfc.img'))\n",
    "segmentation_paths = glob.glob(os.path.join(oasis_dir,'*/FSL_SEG/OAS1_*_MR*_mpr_n*_anon_111_t88_masked_gfc_fseg.img'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3772438",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_id = lambda path : os.path.basename(path).strip('OAS1_')[:4]\n",
    "\n",
    "seg_ids = list(map(path_to_id, segmentation_paths))\n",
    "img_ids = map(path_to_id, image_paths)\n",
    "data = []\n",
    "for img_index,img_id in enumerate(img_ids):\n",
    "    seg_index = seg_ids.index(img_id) if (img_id in seg_ids) else None\n",
    "    seg_path = segmentation_paths[seg_index] if (seg_index is not None) else None\n",
    "    img_path = image_paths[img_index]\n",
    "    data.append(\n",
    "        {\n",
    "            'img' : img_path,\n",
    "             'seg' : seg_path\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c3313",
   "metadata": {},
   "source": [
    "We pretend that only a few segmentations are available.\n",
    "For all but a few items in `data`, we remove the `seg` key.\n",
    "\n",
    "Having partially available keys can be handled nicely by transforms\n",
    "if we set `allow_missing_keys=True` in the transform parameters.\n",
    "That will come up later when we set up transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segs_to_select = 17\n",
    "random.shuffle(data)\n",
    "for image_label_dict in data[:(len(data)-num_segs_to_select)]:\n",
    "    image_label_dict.pop('seg')\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_image(image_array, normalize_by = \"volume\", cmap = None, figsize = (12,12), threshold = None):\n",
    "    \"\"\"\n",
    "    Display three orthogonal slices of the given 3D image.\n",
    "    \n",
    "    image_array is assumed to be of shape (L,W,H)\n",
    "    \n",
    "    if a number is provided for threshold, then pixels for which the value\n",
    "    is below the threshold will be colored red\n",
    "    \"\"\"\n",
    "    if normalize_by == \"slice\" :\n",
    "        vmin = None\n",
    "        vmax = None\n",
    "    elif normalize_by == \"volume\" :\n",
    "        vmin = 0\n",
    "        vmax = image_array.max().item()\n",
    "    else :\n",
    "        raise(ValueError(f\"Invalid value '{normalize_by}' given for normalize_by\"))\n",
    "    \n",
    "    # half-way slices\n",
    "    x,y,z = np.array(image_array.shape)//2\n",
    "    imgs = (image_array[x,:,:], image_array[:,y,:], image_array[:,:,z])\n",
    "    \n",
    "    fig, axs = plt.subplots(1,3,figsize=figsize)\n",
    "    for ax,im in zip(axs,imgs):\n",
    "        ax.axis('off')\n",
    "        ax.imshow(im, origin = 'lower', vmin = vmin, vmax = vmax, cmap=cmap)\n",
    "        \n",
    "        # threshold will be useful when displaying jacobian determinant images;\n",
    "        # we will want to clearly see where the jacobian determinant is negative\n",
    "        if threshold is not None:\n",
    "            red = np.zeros(im.shape+(4,)) # RGBA array\n",
    "            red[im<=threshold] = [1,0,0,1]\n",
    "            ax.imshow(red, origin = 'lower')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "# Uncomment the following to preview a random image\n",
    "# preview_image(monai.transforms.LoadImage(image_only=True, reader='itkreader')(random.choice(data)['img']), figsize=(6,6),  normalize_by=\"slice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812cde6",
   "metadata": {},
   "source": [
    "## Datasets for segmentation network pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07883a",
   "metadata": {},
   "source": [
    "In the DeepAtlas framework, one jointly or alternatively trains a registration network and a segmentation network. It is recommended that the segmentation network be pre-trained first, using whatever little segmentation is available.\n",
    "\n",
    "We select the subset of `data` that has segmentation labels available, and then split that into a training and a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seg_available  =  list(filter(lambda d : 'seg' in d.keys() ,data))\n",
    "data_seg_unavailable = list(filter(lambda d : 'seg' not in d.keys() ,data))\n",
    "\n",
    "data_seg_available_train, data_seg_available_valid = \\\n",
    "    monai.data.utils.partition_dataset(data_seg_available, ratios=(8,2))\n",
    "# Validation of the segmentation network only makes sense if you have enough segmentation labels.\n",
    "# E.g. you should definitely skip validation here if there's just one segmentation label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e12dc4c",
   "metadata": {},
   "source": [
    "Now we set up the chain of transforms that will be used to load images and segmentations for the pre-training of the segmentation network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_seg_available = monai.transforms.Compose(\n",
    "    transforms = [\n",
    "        monai.transforms.LoadImageD(keys=['img','seg'], image_only=True),\n",
    "        monai.transforms.ToTensorD(keys=['img','seg']),\n",
    "        monai.transforms.TransposeD(keys = ['img', 'seg'], indices = (2,1,0)),\n",
    "        monai.transforms.AddChannelD(keys=['img','seg'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Supress the many warnings related to depracation of the Analyze file format\n",
    "# (without this, we would see warnings when the LoadImage transform calls itk to load Analyze files)\n",
    "import itk\n",
    "itk.ProcessObject.SetGlobalWarningDisplay(False)\n",
    "\n",
    "# \"Initializing\" ITK by using it a little as in following line\n",
    "# seems to be necessary to get CacheDataset below to work on the first run.\n",
    "# Related discussion:\n",
    "#   https://discourse.itk.org/t/attributeerror-module-itk-itkclassifierspython-has-no-attribute-swig/3168\n",
    "itk.array_from_image(itk.imread(random.choice(data)['img']));\n",
    "\n",
    "# Uncomment the following line to preview a random image with the transform above applied\n",
    "# preview_image(transform_seg_available(random.choice(data_seg_available))['img'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961cfcf1",
   "metadata": {},
   "source": [
    "And now we define the datasets that use those transforms to load the data. We use `CacheDataset` to take advantage of MONAI's caching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_seg_available_train = monai.data.CacheDataset(\n",
    "    data = data_seg_available_train,\n",
    "    transform = transform_seg_available,\n",
    "    cache_num=8\n",
    ")\n",
    "\n",
    "dataset_seg_available_valid = monai.data.CacheDataset(\n",
    "    data = data_seg_available_valid,\n",
    "    transform = transform_seg_available,\n",
    "    cache_num=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7edd6",
   "metadata": {},
   "source": [
    "## Datasets for training both\n",
    "\n",
    "For the joint/alternative training of the registration and segmentation networks, we want to load _pairs_ of images, along with their segmentation labels when those are available. We create data lists for pairs of images, after reserving some images for validation of the registration network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful not to include any images with segmentation label that were used for seg_net validation.\n",
    "# We don't want those influencing further seg_net training that occurs after pre-training\n",
    "data_without_seg_valid = data_seg_unavailable + data_seg_available_train # Note the order\n",
    "\n",
    "# For validation of reg_net, we prefer not to use the precious data_seg_available_train,\n",
    "# if that's possible. The following split tries to use data_seg_unavailable for the\n",
    "# the validation set, to the extent possible.\n",
    "data_valid, data_train = monai.data.utils.partition_dataset(\n",
    "    data_without_seg_valid, # Note the order\n",
    "    ratios=(2,8), # Note the order\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "def take_data_pairs(data, symmetric=True):\n",
    "    \"\"\"Given a list of dicts that have keys for an image and possibly a segmentation,\n",
    "    return a list of dicts corresponding to *pairs* of images and possible segmentations.\n",
    "    Pairs consisting of a repeated image are not included.\n",
    "    If symmetric is set to True, then for each pair that is included, its reverse is also included\"\"\"\n",
    "    data_pairs = []\n",
    "    for i in range(len(data)):\n",
    "        j_range = range(len(data)) if symmetric else range(i)\n",
    "        for j in j_range:\n",
    "            if j==i: continue\n",
    "            d1 = data[i]\n",
    "            d2 = data[j]\n",
    "            pair = {\n",
    "                'img1' : d1['img'],\n",
    "                'img2' : d2['img']\n",
    "            }\n",
    "            if 'seg' in d1.keys():\n",
    "                pair['seg1'] = d1['seg']\n",
    "            if 'seg' in d2.keys():\n",
    "                pair['seg2'] = d2['seg']\n",
    "            data_pairs.append(pair)\n",
    "    return data_pairs\n",
    "\n",
    "data_pairs_valid = take_data_pairs(data_valid)\n",
    "data_pairs_train = take_data_pairs(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dec66",
   "metadata": {},
   "source": [
    "For the sake of being able to fit everything into graphics memory, we will choose to train the registration and segmentation networks in _alternation_ rather than jointly. When we are doing this we will want a way to load only image pairs that will contribute to segmentation network training-- i.e. image pairs for which at least one of them has a segmentation label available. We create a data list for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs_train_seg_trainable = list(filter(lambda d : 'seg1' in d or 'seg2' in d.keys() ,data_pairs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a020fc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"We have {len(data_pairs_train_seg_trainable)} pairs to train reg_net and seg_net together,\n",
    "  and an additional {len(data_pairs_train) - len(data_pairs_train_seg_trainable)} to train reg_net alone.\"\"\")\n",
    "print(f\"We have {len(data_pairs_valid)} pairs for reg_net validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a02a29",
   "metadata": {},
   "source": [
    "Now we set up the chain of transforms that will be used for loading image pairs.\n",
    "\n",
    "We will concatenate the \"fixed\" and \"moving\" images along the channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pair = monai.transforms.Compose(\n",
    "    transforms = [\n",
    "        monai.transforms.LoadImageD(keys=['img1','seg1', 'img2', 'seg2'], image_only=True,allow_missing_keys=True),\n",
    "        monai.transforms.ToTensorD(keys=['img1','seg1', 'img2', 'seg2'], allow_missing_keys=True),\n",
    "        monai.transforms.TransposeD(keys = ['img1','seg1', 'img2', 'seg2'], indices = (2,1,0), allow_missing_keys=True),\n",
    "        monai.transforms.AddChannelD(keys=['img1','seg1', 'img2', 'seg2'], allow_missing_keys=True),\n",
    "        monai.transforms.ConcatItemsD(keys=['img1', 'img2'], name='img12', dim=0),\n",
    "        monai.transforms.DeleteItemsD(keys=['img1', 'img2'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af437ab6",
   "metadata": {},
   "source": [
    "And now we define the datasets that use the transforms to load the data. Again we use `CacheDataset` to take advantage of MONAI's caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec657e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pairs_train = monai.data.CacheDataset(\n",
    "    data = data_pairs_train,\n",
    "    transform = transform_pair,\n",
    "    cache_num=8\n",
    ")\n",
    "\n",
    "dataset_pairs_valid = monai.data.CacheDataset(\n",
    "    data = data_pairs_valid,\n",
    "    transform = transform_pair,\n",
    "    cache_num=8\n",
    ")\n",
    "\n",
    "dataset_pairs_train_seg_trainable = monai.data.CacheDataset(\n",
    "    data = data_pairs_train_seg_trainable,\n",
    "    transform = transform_pair,\n",
    "    cache_num=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736a2c6",
   "metadata": {},
   "source": [
    "# Defining networks and losses\n",
    "\n",
    "\n",
    "## Segmentation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's not the same as the paper, since the paper does maxpooling for it's seg net,\n",
    "# while this UNet does convolution with stride>1 for the downsampling steps.\n",
    "# Also the paper uses leaky relu activation and this uses prelu.\n",
    "# Also the paper does batch norm while this does instance norm\n",
    "\n",
    "num_segmentation_classes = 4 # Background, csf, white matter, gray matter\n",
    "\n",
    "seg_net = monai.networks.nets.UNet(\n",
    "    3, # spatial dims\n",
    "    1, # input channels\n",
    "    num_segmentation_classes, # output channels\n",
    "    (8,16,16,32,32,64,64), # channel sequence\n",
    "    (1,2,1,2,1,2), # convolutional strides\n",
    "    dropout = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out a forward pass\n",
    "\n",
    "# It's pretty fortunate that the OASIS images have all their spatial dimensions being multiples of 8,\n",
    "# cosnidering that we downsample by a factor of 2 three times in the UNet. Very clean.\n",
    "\n",
    "data_item = random.choice(dataset_seg_available_train)\n",
    "seg_net_example_output = seg_net(data_item['img'].unsqueeze(0))\n",
    "print(f\"Segmentation classes: {torch.unique(data_item['seg'])}\")\n",
    "print(f\"Shape of ground truth label: {data_item['seg'].unsqueeze(0).shape}\")\n",
    "print(f\"Shape of seg_net output: {seg_net_example_output.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b6d54",
   "metadata": {},
   "source": [
    "## Dice loss for segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef665ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = monai.losses.DiceLoss(\n",
    "    include_background=True, # Include background in the multiclass DICE loss: background, CSF, grey, white\n",
    "    to_onehot_y=True, # Our seg labels are single channel images indicating class index, rather than one-hot\n",
    "    softmax=True, # Note that our segmentation network is missing the softmax at the end\n",
    "    reduction=\"mean\" # Follows the paper\n",
    ")\n",
    "\n",
    "# A version of the dice loss with to_onehot_y=False and softmax=False;\n",
    "# will be handy for anatomy loss, for which we often compare two outputs of seg_net\n",
    "dice_loss2 = monai.losses.DiceLoss(\n",
    "    include_background=True,\n",
    "    to_onehot_y=False,\n",
    "    softmax=False,\n",
    "    reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try out the dice loss on the example.\n",
    "\n",
    "# dice_loss(\n",
    "#     seg_net_example_output, # Prediction from seg_net\n",
    "#     data_item['seg'].unsqueeze(0) # Ground truth label\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37debcd7",
   "metadata": {},
   "source": [
    "## Registration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38de75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not exactly identical to the registration network in the paper, but similar\n",
    "# The main difference is that this one doesn't get to the very middle step labeled 1/16\n",
    "# It seems that MONAI UNet design will not stick one block in the middle like we see in fig 3 of the paper\n",
    "\n",
    "reg_net = monai.networks.nets.UNet(\n",
    "    3, # spatial dims\n",
    "    2, # input channels (one for fixed image and one for moving image)\n",
    "    3, # output channels (to represent 3D displacement vector field)\n",
    "    (16,32,32,32,32), # channel sequence\n",
    "    (1,2,2,2), # convolutional strides\n",
    "    dropout = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out a forward pass\n",
    "\n",
    "data_item = random.choice(dataset_pairs_train_seg_trainable)\n",
    "\n",
    "reg_net_example_output = reg_net(data_item['img12'].unsqueeze(0))\n",
    "print(f\"Shape of reg_net output: {reg_net_example_output.shape}\") \n",
    "# The output of the reg_net is assumed to be a displacement vector field\n",
    "# (so e.g. a zero output would be the identity warping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For warping segmentation labels or class predictions, we will want to use nearest neighbor interpolation\n",
    "warp = monai.networks.blocks.Warp(mode=\"bilinear\", padding_mode=\"zeros\")\n",
    "warp_nearest = monai.networks.blocks.Warp(mode=\"nearest\", padding_mode=\"zeros\")\n",
    "\n",
    "# Use example reg_net output to apply warp\n",
    "example_warped_image = warp(\n",
    "    data_item['img12'][[1],:,:,:].unsqueeze(0), # moving image\n",
    "    reg_net_example_output # warping\n",
    ")\n",
    "\n",
    "# Uncomment to preview warped image from forward pass example above\n",
    "# preview_image(example_warped_image[0,0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview deformation as a vector field plot\n",
    "\n",
    "def plot_2D_vector_field(vector_field, downsampling):\n",
    "    \"\"\"vector_field should be a tensor of shape (2,L,W).\n",
    "    The 0 dimension is the two components of the output vectors\"\"\"\n",
    "    downsample2D = monai.networks.layers.factories.Pool['AVG',2](kernel_size=downsampling)\n",
    "    vf_downsampled = downsample2D(vector_field.unsqueeze(0))[0]\n",
    "    plt.quiver(vf_downsampled[0,:,:], vf_downsampled[1,:,:]);\n",
    "    \n",
    "\n",
    "def preview_3D_vector_field(vector_field):\n",
    "    \"\"\"vector_field should be a tensor of shape (3,L,W,H).\n",
    "    The 0 dimension is the three components of the output vectors\"\"\"\n",
    "    \n",
    "    x,y,z = np.array(vector_field.shape[1:])//2 # half-way slices\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.subplot(1,3,1); plt.axis('off')\n",
    "    plot_2D_vector_field(vector_field[[1,2],x,:,:], 4)\n",
    "    plt.subplot(1,3,2); plt.axis('off')\n",
    "    plot_2D_vector_field(vector_field[[0,2],:,y,:], 4)\n",
    "    plt.subplot(1,3,3); plt.axis('off')\n",
    "    plot_2D_vector_field(vector_field[[0,1],:,:,z], 4)\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to preview displacement field from example forward pass of reg_net above\n",
    "# preview_3D_vector_field(reg_net_example_output.detach()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The jacobian determinant provides another nice visual of the effect of a warp,\n",
    "# as well as a way of counting \"folds\" in the warping.\n",
    "# Counting folds can help guide us to use more or less regularization during training\n",
    "def jacobian_determinant(vf):\n",
    "    \"\"\"\n",
    "    Given a displacement vector field vf, compute the jacobian determinant scalar field.\n",
    "    \n",
    "    vf is assumed to be a vector field of shape (3,L,W,H),\n",
    "    and it is interpreted as the displacement field.\n",
    "    So it is defining a discretely sampled map from a subset of 3-space into 3-space,\n",
    "    namely the map that sends point (x,y,z) to the point (x,y,z)+vf[:,x,y,z].\n",
    "    \n",
    "    Returns a numpy array of shape (L,W,H).\n",
    "    \"\"\"\n",
    "\n",
    "    _, L, W, H = vf.shape\n",
    "    \n",
    "    # Compute discrete spatial derivatives\n",
    "    diff_and_trim = lambda array, axis : np.diff(array, axis=axis)[:,:(L-1),:(W-1),:(H-1)]\n",
    "    dx = diff_and_trim(vf, 1)\n",
    "    dy = diff_and_trim(vf, 2)\n",
    "    dz = diff_and_trim(vf, 3)\n",
    "\n",
    "    # Add derivative of identity map\n",
    "    dx[0] += 1\n",
    "    dy[1] += 1\n",
    "    dz[2] += 1\n",
    "\n",
    "    # Compute determinant at each spatial location\n",
    "    det = dx[0]*(dy[1]*dz[2]-dz[1]*dy[2]) - dy[0]*(dx[1]*dz[2]-dz[1]*dx[2]) + dz[0]*(dx[1]*dy[2]-dy[1]*dx[2])\n",
    "\n",
    "    return det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b681022",
   "metadata": {},
   "source": [
    "## Image similarity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local normalized cross-correlation loss.\n",
    "# The paper does a global NCC, but MONAI provides local and it's easy to use.\n",
    "# Another difference from the paper: instead of returning 1-LNCC this will return -LNCC\n",
    "\n",
    "lncc_loss = monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
    "    ndim=3, # this keyword argument will soon be renamed to spatial_dims\n",
    "    kernel_size=3,\n",
    "    kernel_type='rectangular',\n",
    "    reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try out the image similarity loss on the example.\n",
    "\n",
    "# lncc_loss(\n",
    "#     example_warped_image, # registered image\n",
    "#     data_item['img12'][[0],:,:,:].unsqueeze(0) # target (\"fixed image\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da58e16",
   "metadata": {},
   "source": [
    "## Regularization loss for registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bending_loss = monai.losses.BendingEnergyLoss()\n",
    "\n",
    "# Uncomment to try out the bending energy loss on the example\n",
    "# bending_loss(reg_net_example_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc030353",
   "metadata": {},
   "source": [
    "# Segmentation network pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72730113",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_seg_available_train = monai.data.DataLoader(\n",
    "    dataset_seg_available_train,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_seg_available_valid = monai.data.DataLoader(\n",
    "    dataset_seg_available_valid,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc25b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training cell, uncomment when you want to train\n",
    "\n",
    "seg_net.to('cuda')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(seg_net.parameters(), learning_rate)\n",
    "\n",
    "max_epochs = 45\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch_number in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    seg_net.train()\n",
    "    losses = []\n",
    "    for batch in dataloader_seg_available_train:\n",
    "        imgs = batch['img'].to('cuda')\n",
    "        true_segs = batch['seg'].to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_segs = seg_net(imgs)\n",
    "        loss = dice_loss(predicted_segs, true_segs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    training_loss = np.mean(losses)\n",
    "    \n",
    "    seg_net.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader_seg_available_valid:\n",
    "            imgs = batch['img'].to('cuda')\n",
    "            true_segs = batch['seg'].to('cuda')\n",
    "            predicted_segs = seg_net(imgs)\n",
    "            loss = dice_loss(predicted_segs, true_segs)\n",
    "            losses.append(loss.item())\n",
    "        validation_loss = np.mean(losses)\n",
    "        \n",
    "    print(f\"training loss: {training_loss}, validation loss: {validation_loss}\")\n",
    "    preview_image(torch.argmax(torch.softmax(seg_net(dataset_seg_available_valid[0]['img'].unsqueeze(0).cuda()),dim=1),dim=1, keepdim=True)[0,0].cpu(), figsize=(6,6))\n",
    "    \n",
    "    training_losses.append(training_loss)\n",
    "    validation_losses.append(validation_loss)\n",
    "\n",
    "# Clean up to keep cuda memory usage under control\n",
    "del loss, predicted_segs, true_segs, imgs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses\n",
    "\n",
    "plt.plot(training_losses, label=\"training\")\n",
    "plt.plot(validation_losses, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mean dice loss')\n",
    "plt.title('seg_net pretraining')\n",
    "plt.savefig('seg_net_pretrained_losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d031795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; SAVE\n",
    "torch.save(seg_net.state_dict(),'seg_net_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfab83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; LOAD\n",
    "seg_net.load_state_dict(torch.load('seg_net_pretrained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try out pretrained seg net on one of the validation images\n",
    "\n",
    "seg_net.to('cuda')\n",
    "\n",
    "data_item = random.choice(dataset_seg_available_valid)\n",
    "test_input = data_item['img']\n",
    "test_seg_true = data_item['seg']\n",
    "seg_net.eval()\n",
    "with torch.no_grad():\n",
    "    test_seg_predicted = seg_net(test_input.unsqueeze(0).cuda()).cpu()\n",
    "\n",
    "# Original image from validation set\n",
    "preview_image(test_input[0])\n",
    "# Ground truth segmentation\n",
    "preview_image(test_seg_true[0])\n",
    "# Our predicted segmentation\n",
    "preview_image(torch.argmax(torch.softmax(test_seg_predicted,dim=1),dim=1, keepdim=True)[0,0])\n",
    "\n",
    "del test_seg_predicted\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ef58f",
   "metadata": {},
   "source": [
    "# Training both in alternation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0700cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_training(network_to_train, network_to_not_train):\n",
    "    \"\"\"\n",
    "        Switch out of training one network and into training another\n",
    "    \"\"\"\n",
    "\n",
    "    for param in network_to_not_train.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for param in network_to_train.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    network_to_not_train.eval()\n",
    "    network_to_train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute the ingredients of the final loss computation for reg_net\n",
    "\n",
    "regularization_loss = bending_loss\n",
    "\n",
    "def similarity_loss(displacement_field, image_pair):\n",
    "    \"\"\" Accepts a batch of displacement fields, shape (B,3,L,W,H)\n",
    "        and a batch of image pairs, shape (B,2,L,W,H) \"\"\"\n",
    "    warped_img2 = warp(image_pair[:,[1],:,:,:], displacement_field)\n",
    "    return lncc_loss(\n",
    "        warped_img2, # prediction\n",
    "        image_pair[:,[0],:,:,:] # target\n",
    "    )\n",
    "\n",
    "def anatomy_loss(displacement_field, image_pair, seg_net, gt_seg1 = None, gt_seg2 = None):\n",
    "    \"\"\" \n",
    "    Accepts a batch of displacement fields, shape (B,3,L,W,H)\n",
    "    and a batch of image pairs, shape (B,2,L,W,H).\n",
    "    seg_net is the model used to segment an image,\n",
    "      mapping (B,1,L,W,H) to (B,C,L,W,H) where C is the number of segmentation classes\n",
    "    gt_seg1 and gt_seg2 are ground truth segmentations for the images in image_pair, if ground truth is available;\n",
    "      if unavailable then they can be None.\n",
    "      gt_seg1 and gt_seg2 are expected to be in the form of class labels, with shape (B,1,L,W,H)\n",
    "    \"\"\"\n",
    "    if gt_seg1 is not None:\n",
    "        # ground truth seg of target image\n",
    "        seg1 = monai.networks.one_hot(\n",
    "            gt_seg1 , num_segmentation_classes\n",
    "        )\n",
    "    else:\n",
    "        # seg_net on target image, \"noisy ground truth\"\n",
    "        seg1 = seg_net(image_pair[:,[0],:,:,:]).softmax(dim=1) \n",
    "\n",
    "    if gt_seg2 is not None:\n",
    "        # ground truth seg of moving image\n",
    "        seg2 = monai.networks.one_hot(\n",
    "            gt_seg2 , num_segmentation_classes\n",
    "        )\n",
    "    else:\n",
    "        # seg_net on moving image, \"noisy ground truth\"\n",
    "        seg2 = seg_net(image_pair[:,[1],:,:,:]).softmax(dim=1)\n",
    "\n",
    "    # seg1 and seg2 should now be in the form of one-hot class probabilities\n",
    "\n",
    "    return dice_loss2(\n",
    "        warp_nearest(seg2, displacement_field), # warp of moving image segmentation\n",
    "        seg1 # target image segmentation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8af785",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training cell, uncomment when you want to train\n",
    "\n",
    "seg_net.to('cuda')\n",
    "reg_net.to('cuda')\n",
    "\n",
    "dataloader_pairs_train = monai.data.DataLoader(\n",
    "    dataset_pairs_train,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_pairs_valid = monai.data.DataLoader(\n",
    "    dataset_pairs_valid,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    shuffle=True # Weird to shuffle validation data, but makes sense if you just take a small sample each time\n",
    "                 # (which I've been doing because it would take way too long to go through all validation pairs)\n",
    ")\n",
    "\n",
    "dataloader_pairs_train_seg_trainable = monai.data.DataLoader(\n",
    "    dataset_pairs_train_seg_trainable,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_reg = 5e-4\n",
    "optimizer_reg = torch.optim.Adam(reg_net.parameters(), learning_rate_reg)\n",
    "\n",
    "learning_rate_seg = 1e-4\n",
    "optimizer_seg = torch.optim.Adam(seg_net.parameters(), learning_rate_seg)\n",
    "\n",
    "# Weights of various registration losses, relative to the image similarity loss\n",
    "lambda_r  = 0.2 # regularization loss weight\n",
    "lambda_a  = 2.0 # anatomy loss weight\n",
    "lambda_sp = 3.0 # supervised segmentation loss weight\n",
    "\n",
    "\n",
    "# max_epochs = 40\n",
    "# reg_phase_training_batches_per_epoch = 40\n",
    "# seg_phase_training_batches_per_epoch = 10\n",
    "# reg_phase_num_validation_batches_to_use = 40\n",
    "\n",
    "max_epochs = 30\n",
    "reg_phase_training_batches_per_epoch = 1\n",
    "seg_phase_training_batches_per_epoch = 1\n",
    "reg_phase_num_validation_batches_to_use = 1\n",
    "\n",
    "training_losses_reg = []\n",
    "validation_losses_reg = []\n",
    "training_losses_seg = []\n",
    "validation_losses_seg = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch_number in range(max_epochs):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    #         reg_net training phase\n",
    "    # ------------------------------------------------\n",
    "    \n",
    "    # Keep computational graph in memory for reg_net, but not for seg_net, and do reg_net.train()\n",
    "    swap_training(reg_net, seg_net)\n",
    "    \n",
    "    losses = []\n",
    "    for batch_number, batch in enumerate(dataloader_pairs_train):\n",
    "        if batch_number >= reg_phase_training_batches_per_epoch : break;\n",
    "        img12 = batch['img12'].to('cuda')\n",
    "        \n",
    "\n",
    "        optimizer_reg.zero_grad()\n",
    "        displacement_field12 = reg_net(img12)\n",
    "        \n",
    "        loss_sim = similarity_loss(displacement_field12, img12)\n",
    "        \n",
    "        loss_reg = regularization_loss(displacement_field12)\n",
    "        \n",
    "        gt_seg1 = batch['seg1'].to('cuda') if 'seg1' in batch.keys() else None\n",
    "        gt_seg2 = batch['seg2'].to('cuda') if 'seg2' in batch.keys() else None\n",
    "        loss_ana = anatomy_loss(displacement_field12, img12, seg_net, gt_seg1, gt_seg2)\n",
    "        \n",
    "        loss = loss_sim + lambda_r * loss_reg + lambda_a * loss_ana\n",
    "        \n",
    "        # Start with this if you want to introduce inverse consistency loss\n",
    "#         img21 = img12[:,[1,0],:,:,:]\n",
    "#         displacement_field21 = reg_net(img21)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_reg.step()\n",
    "        \n",
    "        \n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    training_loss = np.mean(losses)\n",
    "    \n",
    "    \n",
    "    \n",
    "    reg_net.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_number, batch in enumerate(dataloader_pairs_valid):\n",
    "            if batch_number >= reg_phase_num_validation_batches_to_use: break\n",
    "            img12 = batch['img12'].to('cuda')\n",
    "            # img21 = img12[:,[1,0],:,:,:]\n",
    "            displacement_fields12 = reg_net(img12)\n",
    "            warped_img2 = warp(img12[:,[1],:,:,:], displacement_fields12)\n",
    "            loss_similarity = lncc_loss(\n",
    "                warped_img2, # prediciton\n",
    "                img12[:,[0],:,:,:] # target\n",
    "            )\n",
    "            loss_regularization = bending_loss(displacement_fields12)\n",
    "\n",
    "            if 'seg1' in batch.keys():\n",
    "                seg1 = monai.networks.one_hot(\n",
    "                    batch['seg1'].to('cuda') , num_segmentation_classes\n",
    "                )\n",
    "            else:\n",
    "                seg1 = seg_net(img12[:,[0],:,:,:]).softmax(dim=1) \n",
    "\n",
    "            if 'seg2' in batch.keys():\n",
    "                # ground truth seg of moving image\n",
    "                seg2 = monai.networks.one_hot(\n",
    "                    batch['seg2'].to('cuda') , num_segmentation_classes\n",
    "                )\n",
    "            else:\n",
    "                seg2 = seg_net(img12[:,[1],:,:,:]).softmax(dim=1)\n",
    "\n",
    "            loss_anatomy = dice_loss2(\n",
    "                warp_nearest(seg2, displacement_fields12), # warp of moving image segmentation\n",
    "                seg1 # target image segmentation\n",
    "            )\n",
    "            \n",
    "            loss = loss_similarity + lambda_r * loss_regularization + lambda_a * loss_anatomy\n",
    "            losses.append(loss.item())\n",
    "        validation_loss = np.mean(losses)\n",
    "        \n",
    "    print(\n",
    "        f\"Epoch {epoch_number+1}/{max_epochs} summary:\",\n",
    "        f\"  reg training loss: {training_loss}\",\n",
    "        f\"  reg validation loss: {validation_loss}\",\n",
    "        sep = '\\n'\n",
    "    )\n",
    "\n",
    "    training_losses_reg.append(training_loss)\n",
    "    validation_losses_reg.append(validation_loss)\n",
    "    \n",
    "    # Free up cuda memory\n",
    "    del loss, loss_similarity, loss_regularization, loss_anatomy,\\\n",
    "        warped_img2, displacement_field12,\\\n",
    "        img12, seg1, seg2\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    #         seg_net training phase\n",
    "    # ------------------------------------------------\n",
    "    \n",
    "    # Keep computational graph in memory for seg_net, but not for reg_net, and do seg_net.train()\n",
    "    swap_training(seg_net, reg_net)\n",
    "    \n",
    "    losses = []\n",
    "    for batch_number, batch in enumerate(dataloader_pairs_train_seg_trainable):\n",
    "        if batch_number >= seg_phase_training_batches_per_epoch : break;   \n",
    "            \n",
    "        optimizer_seg.zero_grad()\n",
    "        \n",
    "        img12 = batch['img12'].to('cuda')\n",
    "        \n",
    "        displacement_fields = reg_net(img12)\n",
    "        seg1_predicted = seg_net(img12[:,[0],:,:,:]).softmax(dim=1)\n",
    "        seg2_predicted = seg_net(img12[:,[1],:,:,:]).softmax(dim=1)\n",
    "        \n",
    "        if 'seg1' in batch.keys() and 'seg2' in batch.keys():\n",
    "            seg1 = monai.networks.one_hot(batch['seg1'].to('cuda') , num_segmentation_classes)\n",
    "            seg2 = monai.networks.one_hot(batch['seg2'].to('cuda') , num_segmentation_classes)\n",
    "            loss_supervised = dice_loss2( seg1_predicted , seg1 ) + dice_loss2( seg2_predicted , seg2 )\n",
    "            # The above supervised loss looks a bit different from the one in the paper;\n",
    "            # this is because the set of image pairs that we iterate over is not symmetric,\n",
    "            # so a label that shows up as seg2 will never also show up as seg1, and vice versa\n",
    "            \n",
    "        elif 'seg1' in batch.keys(): # seg1 available, but no seg2\n",
    "            seg1 = monai.networks.one_hot(batch['seg1'].to('cuda') , num_segmentation_classes)\n",
    "            loss_supervised = dice_loss2( seg1_predicted , seg1 )\n",
    "            seg2 = seg2_predicted # Use this in anatomy loss\n",
    "        \n",
    "        else: # seg2 available, but no seg1\n",
    "            assert('seg2' in batch.keys())\n",
    "            seg2 = monai.networks.one_hot(batch['seg2'].to('cuda') , num_segmentation_classes)\n",
    "            loss_supervised = dice_loss2( seg2_predicted , seg2 )\n",
    "            seg1 = seg1_predicted # Use this in anatomy loss\n",
    "            \n",
    "        \n",
    "        # seg1 and seg2 should now be in the form of one-hot class probabilities\n",
    "        loss_anatomy = dice_loss2( warp_nearest(seg2, displacement_fields), seg1 )\n",
    "        \n",
    "        loss = lambda_a * loss_anatomy + lambda_sp * loss_supervised\n",
    "        loss.backward()\n",
    "        optimizer_seg.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "    training_loss = np.mean(losses)\n",
    "    \n",
    "    # The following validation loop would not do anything in the case\n",
    "    # where there is just one segmentation available,\n",
    "    # because data_seg_available_valid would be empty.\n",
    "    seg_net.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader_seg_available_valid:\n",
    "            imgs = batch['img'].to('cuda')\n",
    "            true_segs = batch['seg'].to('cuda')\n",
    "            predicted_segs = seg_net(imgs)\n",
    "            loss = dice_loss(predicted_segs, true_segs)\n",
    "            losses.append(loss.item())\n",
    "        validation_loss = np.mean(losses)\n",
    "        \n",
    "    print(\n",
    "        f\"  seg training loss: {training_loss}\",\n",
    "        f\"  seg validation loss: {validation_loss}\",\n",
    "        sep = '\\n'\n",
    "    )\n",
    "    \n",
    "    training_losses_seg.append(training_loss)\n",
    "    validation_losses_seg.append(validation_loss)\n",
    "    \n",
    "    # Free up cuda memory\n",
    "    del imgs, true_segs, predicted_segs, loss, seg1, seg2,\\\n",
    "        displacement_fields, img12, loss_supervised, loss_anatomy,\\\n",
    "        seg1_predicted, seg2_predicted\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050dd7b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation losses\n",
    "\n",
    "plt.plot(training_losses_reg, label=\"training\")\n",
    "plt.plot(validation_losses_reg, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Alternate training: registration loss')\n",
    "plt.savefig('reg_net_losses.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(training_losses_seg, label=\"training\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Alternate training: segmentation loss (training)')\n",
    "plt.savefig('seg_net_training_losses.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(validation_losses_seg, label=\"validation\", color='orange')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('validation loss')\n",
    "plt.title('Alternate training: segmentation loss (validation)')\n",
    "plt.savefig('seg_net_validation_losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76998a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; SAVE\n",
    "torch.save(seg_net.state_dict(),'seg_net.pth')\n",
    "torch.save(reg_net.state_dict(),'reg_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; LOAD\n",
    "# seg_net.load_state_dict(torch.load('seg_net.pth'))\n",
    "# reg_net.load_state_dict(torch.load('reg_net.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801a48e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment to try out reg net on one of the validation pairs\n",
    "\n",
    "reg_net.to('cuda')\n",
    "reg_net.eval()\n",
    "\n",
    "data_item = random.choice(dataset_pairs_valid)\n",
    "img12 = data_item['img12'].unsqueeze(0).to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    reg_net_example_output = reg_net(img12)\n",
    "\n",
    "example_warped_image = warp(\n",
    "    img12[:,[1],:,:,:], # moving image\n",
    "    reg_net_example_output # warping\n",
    ")\n",
    "\n",
    "# Uncomment to preview displacement field and warped image\n",
    "print(\"moving image:\")\n",
    "preview_image(img12[0,1,:,:,:].cpu(), normalize_by=\"slice\", cmap='gray')\n",
    "print(\"target image:\")\n",
    "preview_image(img12[0,0,:,:,:].cpu(), normalize_by=\"slice\", cmap='gray')\n",
    "print(\"warped moving image:\")\n",
    "preview_image(example_warped_image[0,0].cpu(), normalize_by=\"slice\", cmap='gray')\n",
    "print(\"deformation field:\")\n",
    "preview_3D_vector_field(reg_net_example_output.cpu().detach()[0])\n",
    "print(\"jacobian determinant:\")\n",
    "det = jacobian_determinant(reg_net_example_output.cpu().detach()[0])\n",
    "preview_image(det, normalize_by='slice', threshold=0)\n",
    "loss = lncc_loss(example_warped_image,img12[:,[0],:,:,:]).item()\n",
    "print(f\"Similarity loss: {loss}\")\n",
    "print(f\"number of folds: {(det<=0).sum()}\")\n",
    "\n",
    "del reg_net_example_output, img12, example_warped_image\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try out seg net on one of the validation images\n",
    "\n",
    "seg_net.to('cuda')\n",
    "\n",
    "data_item = random.choice(dataset_seg_available_valid)\n",
    "test_input = data_item['img']\n",
    "test_seg_true = data_item['seg']\n",
    "seg_net.eval()\n",
    "with torch.no_grad():\n",
    "    test_seg_predicted = seg_net(test_input.unsqueeze(0).cuda()).cpu()\n",
    "    loss = dice_loss(test_seg_predicted, test_seg_true.unsqueeze(0)).item()\n",
    "\n",
    "print(\"original image from validation set:\")\n",
    "preview_image(test_input[0], normalize_by=\"slice\", cmap='gray')\n",
    "print(\"ground truth segmentation:\")\n",
    "preview_image(test_seg_true[0])\n",
    "print(\"our predicted segmentation:\")\n",
    "preview_image(torch.argmax(torch.softmax(test_seg_predicted,dim=1),dim=1, keepdim=True)[0,0])\n",
    "print(f\"dice loss: {loss}\")\n",
    "\n",
    "del test_seg_predicted\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we evaluate the regularity of reg_net\n",
    "# by sampling some image pairs from the validation set\n",
    "# and counting folds\n",
    "\n",
    "dataloader_pairs_valid = monai.data.DataLoader(\n",
    "    dataset_pairs_valid,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    "    shuffle=True # Weird to shuffle validation data, but makes sense if you just take a small sample each time\n",
    "                 # (which I've been doing because it would take way too long to go through all validation pairs)\n",
    ")\n",
    "\n",
    "num_batches = 200 # sample this many batches\n",
    "\n",
    "fold_counts = []\n",
    "reg_net.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_number, batch in enumerate(dataloader_pairs_valid):\n",
    "        if batch_number >= num_batches: break\n",
    "        img12 = batch['img12'].to('cuda')\n",
    "        displacement_fields = reg_net(img12)\n",
    "        for displacement_field in displacement_fields:\n",
    "            det = jacobian_determinant(displacement_field.cpu())\n",
    "            num_folds = (det <= 0).sum()\n",
    "            fold_counts.append(num_folds)\n",
    "        \n",
    "del img12, displacement_fields\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "plt.hist(fold_counts)\n",
    "plt.title(\"Number of folds\")\n",
    "plt.savefig('folds_histogram.png')\n",
    "plt.show()\n",
    "print(f\"Mean fold count: {np.mean(fold_counts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
