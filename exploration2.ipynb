{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d697b614",
   "metadata": {},
   "source": [
    "# DeepAtlas\n",
    "\n",
    "https://arxiv.org/abs/1904.08465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "576b9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import torch\n",
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad668fbc",
   "metadata": {},
   "source": [
    "# Loading OASIS images\n",
    "\n",
    "See [fact sheet here](https://www.oasis-brains.org/files/oasis_cross-sectional_facts.pdf) for info on OASIS-1 dataset.\n",
    "\n",
    "## Filepaths\n",
    "\n",
    "We already have a segmentaton for each image. These segmentations were originally done by an algorithm:\n",
    "\n",
    "*Segmentation of brain MR images through a hidden Markov random field model and the expectation maximization algorithm*\\\n",
    "2001 Zhang, Brady, Smith\\\n",
    "[link](https://pubmed.ncbi.nlm.nih.gov/11293691/)\n",
    "\n",
    "The situation in which we'd want to apply DeepAtlas is one where we have just a handful of carefully created manual segmengations. We will simulate this situation by pretending that some of the segmentation labels don't exist, and ignoring the fact that the segmentations we have come from an algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ada63a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "oasis_dir = \"/home/ebrahim/data/OASIS-1/\"\n",
    "image_paths = glob.glob(os.path.join(oasis_dir,'*/PROCESSED/MPRAGE/T88_111/OAS1_*_MR*_mpr_n*_anon_111_t88_masked_gfc.img'))\n",
    "segmentation_paths = glob.glob(os.path.join(oasis_dir,'*/FSL_SEG/OAS1_*_MR*_mpr_n*_anon_111_t88_masked_gfc_fseg.img'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3772438",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_id = lambda path : os.path.basename(path).strip('OAS1_')[:4]\n",
    "\n",
    "seg_ids = list(map(path_to_id, segmentation_paths))\n",
    "img_ids = map(path_to_id, image_paths)\n",
    "data = []\n",
    "for img_index,img_id in enumerate(img_ids):\n",
    "    seg_index = seg_ids.index(img_id) if (img_id in seg_ids) else None\n",
    "    seg_path = segmentation_paths[seg_index] if (seg_index is not None) else None\n",
    "    img_path = image_paths[img_index]\n",
    "    data.append(\n",
    "        {\n",
    "            'img' : img_path,\n",
    "             'seg' : seg_path\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c3313",
   "metadata": {},
   "source": [
    "We pretend that only a few segmentations are available.\n",
    "For all but a few items in `data`, we remove the `seg` key.\n",
    "\n",
    "Having partially available keys can be handled nicely by transforms\n",
    "if we set `allow_missing_keys=True` in the transform parameters.\n",
    "That will come up later when we set up transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b253053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segs_to_select = 17\n",
    "random.shuffle(data)\n",
    "for image_label_dict in data[:(len(data)-num_segs_to_select)]:\n",
    "    image_label_dict.pop('seg')\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baab9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_image(image_array):\n",
    "    x,y,z = np.array(image_array.shape)//2 # half-way slices\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(1,3,1); plt.axis('off')\n",
    "    plt.imshow(image_array[x,:,:], origin = 'lower')\n",
    "    plt.subplot(1,3,2); plt.axis('off')\n",
    "    plt.imshow(image_array[:,y,:], origin = 'lower')\n",
    "    plt.subplot(1,3,3); plt.axis('off')\n",
    "    plt.imshow(image_array[:,:,z], origin = 'lower')\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment the following to preview a random image\n",
    "# preview_image(monai.transforms.LoadImage(image_only=True)(random.choice(data)['img']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812cde6",
   "metadata": {},
   "source": [
    "## Datasets for segmentation network pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07883a",
   "metadata": {},
   "source": [
    "In the DeepAtlas framework, one jointly or alternatively trains a registration network and a segmentation network. It is recommended that the segmentation network be pre-trained first, using whatever little segmentation is available.\n",
    "\n",
    "We select the subset of `data` that has segmentation labels available, and then split that into a training and a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1331a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seg_available  =  list(filter(lambda d : 'seg' in d.keys() ,data))\n",
    "data_seg_unavailable = list(filter(lambda d : 'seg' not in d.keys() ,data))\n",
    "\n",
    "data_seg_available_train, data_seg_available_valid = \\\n",
    "    monai.data.utils.partition_dataset(data_seg_available, ratios=(8,2))\n",
    "# Validation of the segmentation network only makes sense if you have enough segmentation labels.\n",
    "# E.g. you should definitely skip validation here if there's just one segmentation label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e12dc4c",
   "metadata": {},
   "source": [
    "Now we set up the chain of transforms that will be used to load images and segmentations for the pre-training of the segmentation network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd7c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_seg_available = monai.transforms.Compose(\n",
    "    transforms = [\n",
    "        monai.transforms.LoadImageD(keys=['img','seg'], image_only=True),\n",
    "        monai.transforms.ToTensorD(keys=['img','seg']),\n",
    "        monai.transforms.AddChannelD(keys=['img','seg'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Supress the many warnings related to depracation of the Analyze file format\n",
    "# (without this, we would see warnings when the LoadImage transform calls itk to load Analyze files)\n",
    "import itk\n",
    "itk.ProcessObject.SetGlobalWarningDisplay(False)\n",
    "\n",
    "# \"Initializing\" ITK by using it a little as in following line\n",
    "# seems to be necessary to get CacheDataset below to work on the first run.\n",
    "# Related discussion:\n",
    "#   https://discourse.itk.org/t/attributeerror-module-itk-itkclassifierspython-has-no-attribute-swig/3168\n",
    "itk.array_from_image(itk.imread(random.choice(data)['img']));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961cfcf1",
   "metadata": {},
   "source": [
    "And now we define the datasets that use those transforms to load the data. We use `CacheDataset` to take advantage of MONAI's caching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba96fd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 3620.68it/s]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 158.96it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_seg_available_train = monai.data.CacheDataset(\n",
    "    data = data_seg_available_train,\n",
    "    transform = transform_seg_available,\n",
    ")\n",
    "\n",
    "dataset_seg_available_valid = monai.data.CacheDataset(\n",
    "    data = data_seg_available_valid,\n",
    "    transform = transform_seg_available,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7edd6",
   "metadata": {},
   "source": [
    "## Datasets for training both\n",
    "\n",
    "For the joint/alternative training of the registration and segmentation networks, we want to load _pairs_ of images, along with their segmentation labels when those are available. We create data lists for pairs of images, after reserving some images for validation of the registration network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1790634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful not to include any images with segmentation label that were used for seg_net validation.\n",
    "# We don't want those influencing further seg_net training that occurs after pre-training\n",
    "data_without_seg_valid = data_seg_unavailable + data_seg_available_train # Note the order\n",
    "\n",
    "# For validation of reg_net, we prefer not to use the precious data_seg_available_train,\n",
    "# if that's possible. The following split tries to use data_seg_unavailable for the\n",
    "# the validation set, to the extent possible.\n",
    "data_valid, data_train = monai.data.utils.partition_dataset(\n",
    "    data_without_seg_valid, # Note the order\n",
    "    ratios=(2,8), # Note the order\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "def take_data_pairs(data):\n",
    "    \"\"\"Given a list of dicts that have keys for an image and possibly a segmentation,\n",
    "    return a list of dicts corresponding to *pairs* of images and possible segmentations.\n",
    "    Pairs consisting of a repeated image are not included, and only one ordering is\n",
    "    included between a pair and its reverse.\"\"\"\n",
    "    data_pairs = []\n",
    "    for i in range(len(data)):\n",
    "        # We will not allow reverse pairs, since we plan to handle both \"directions\" in each training step\n",
    "        # (this allows us to introduce inverse consistency loss, if we want to)\n",
    "        for j in range(i):\n",
    "            d1 = data[i]\n",
    "            d2 = data[j]\n",
    "            pair = {\n",
    "                'img1' : d1['img'],\n",
    "                'img2' : d2['img']\n",
    "            }\n",
    "            if 'seg' in d1.keys():\n",
    "                pair['seg1'] = d1['seg']\n",
    "            if 'seg' in d2.keys():\n",
    "                pair['seg2'] = d2['seg']\n",
    "            data_pairs.append(pair)\n",
    "    return data_pairs\n",
    "\n",
    "data_pairs_valid = take_data_pairs(data_valid)\n",
    "data_pairs_train = take_data_pairs(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dec66",
   "metadata": {},
   "source": [
    "For the sake of being able to fit everything into graphics memory, we will choose to train the registration and segmentation networks in _alternation_ rather than jointly. When we are doing this we will want a way to load only image pairs that will contribute to segmentation network training-- i.e. image pairs for which at least one of them has a segmentation label available. We create a data list for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46e116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs_train_seg_trainable = list(filter(lambda d : 'seg1' in d or 'seg2' in d.keys() ,data_pairs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a020fc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 4725 pairs to train reg_net and seg_net together,\n",
      "  and an additional 54615 to train reg_net alone.\n",
      "We have 3655 pairs for reg_net validation.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"We have {len(data_pairs_train_seg_trainable)} pairs to train reg_net and seg_net together,\n",
    "  and an additional {len(data_pairs_train) - len(data_pairs_train_seg_trainable)} to train reg_net alone.\"\"\")\n",
    "print(f\"We have {len(data_pairs_valid)} pairs for reg_net validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a02a29",
   "metadata": {},
   "source": [
    "Now we set up the chain of transforms that will be used for loading image pairs.\n",
    "\n",
    "We will concatenate the \"fixed\" and \"moving\" images along the channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4989d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pair = monai.transforms.Compose(\n",
    "    transforms = [\n",
    "        monai.transforms.LoadImageD(keys=['img1','seg1', 'img2', 'seg2'], image_only=True,allow_missing_keys=True),\n",
    "        monai.transforms.ToTensorD(keys=['img1','seg1', 'img2', 'seg2'], allow_missing_keys=True),\n",
    "        monai.transforms.AddChannelD(keys=['img1','seg1', 'img2', 'seg2'], allow_missing_keys=True),\n",
    "        monai.transforms.ConcatItemsD(keys=['img1', 'img2'], name='img12', dim=0),\n",
    "        monai.transforms.DeleteItemsD(keys=['img1', 'img2'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af437ab6",
   "metadata": {},
   "source": [
    "And now we define the datasets that use the transforms to load the data. Again we use `CacheDataset` to take advantage of MONAI's caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ec657e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 287.11it/s]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 422.95it/s]\n",
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 251.16it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_pairs_train = monai.data.CacheDataset(\n",
    "    data = data_pairs_train,\n",
    "    transform = transform_pair,\n",
    "    cache_num=8\n",
    ")\n",
    "\n",
    "dataset_pairs_valid = monai.data.CacheDataset(\n",
    "    data = data_pairs_valid,\n",
    "    transform = transform_pair,\n",
    "    cache_num=8\n",
    ")\n",
    "\n",
    "dataset_pairs_train_seg_trainable = monai.data.CacheDataset(\n",
    "    data = data_pairs_train_seg_trainable,\n",
    "    transform = transform_pair,\n",
    "    cache_num=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736a2c6",
   "metadata": {},
   "source": [
    "# Defining networks and losses\n",
    "\n",
    "\n",
    "## Segmentation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b53b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's not the same as the paper, since the paper does maxpooling for it's seg net,\n",
    "# while this UNet does convolution with stride>1 for the downsampling steps.\n",
    "# Also the paper uses leaky relu activation and this uses prelu.\n",
    "# Also the paper does batch norm while this does instance norm\n",
    "\n",
    "seg_net = monai.networks.nets.UNet(\n",
    "    3, # spatial dims\n",
    "    1, # input channels\n",
    "    4, # output channels (4 segmentation classes)\n",
    "    (8,16,16,32,32,64,64), # channel sequence\n",
    "    (1,2,1,2,1,2) # convolutional strides\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f1e6f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation classes: tensor([0., 1., 2., 3.])\n",
      "Shape of ground truth label: torch.Size([1, 1, 176, 208, 176])\n",
      "Shape of seg_net output: torch.Size([1, 4, 176, 208, 176])\n"
     ]
    }
   ],
   "source": [
    "# Try out a forward pass\n",
    "\n",
    "# It's pretty fortunate that the OASIS images have all their spatial dimensions being multiples of 8,\n",
    "# cosnidering that we downsample by a factor of 2 three time in the UNet. Very clean.\n",
    "\n",
    "data_item = random.choice(dataset_seg_available_train)\n",
    "seg_net_example_output = seg_net(data_item['img'].unsqueeze(0))\n",
    "print(f\"Segmentation classes: {torch.unique(data_item['seg'])}\")\n",
    "print(f\"Shape of ground truth label: {data_item['seg'].unsqueeze(0).shape}\")\n",
    "print(f\"Shape of seg_net output: {seg_net_example_output.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b6d54",
   "metadata": {},
   "source": [
    "## Dice loss for segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ef665ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = monai.losses.DiceLoss(\n",
    "    include_background=True, # Include background in the multiclass DICE loss: background, CSF, grey, white\n",
    "    to_onehot_y=True, # Our seg labels are single channel images indicating class index, rather than one-hot\n",
    "    softmax=True, # Note that our segmentation network is missing the softmax at the end\n",
    "    reduction=\"mean\" # Follows the paper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f31e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try out the dice loss on the example.\n",
    "\n",
    "# dice_loss(\n",
    "#     seg_net_example_output, # Prediction from seg_net\n",
    "#     data_item['seg'].unsqueeze(0) # Ground truth label\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37debcd7",
   "metadata": {},
   "source": [
    "## Registration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d38de75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not exactly identical to the registration network in the paper, but similar\n",
    "# The main difference is that this one doesn't get to the very middle step labeled 1/16\n",
    "# It seems that MONAI UNet design will not stick one block in the middle like we see in fig 3 of the paper\n",
    "\n",
    "reg_net = monai.networks.nets.UNet(\n",
    "    3, # spatial dims\n",
    "    2, # input channels (one for fixed image and one for moving image)\n",
    "    3, # output channels (to represent 3D displacement vector field)\n",
    "    (16,32,32,32,32), # channel sequence\n",
    "    (1,2,2,2,2) # convolutional strides\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09ff16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reg_net output: torch.Size([1, 3, 176, 208, 176])\n"
     ]
    }
   ],
   "source": [
    "# Try out a forward pass\n",
    "\n",
    "data_item = random.choice(dataset_pairs_train_seg_trainable)\n",
    "\n",
    "reg_net_example_output = reg_net(data_item['img12'].unsqueeze(0))\n",
    "print(f\"Shape of reg_net output: {reg_net_example_output.shape}\") \n",
    "# The output of the reg_net is assumed to be a displacement vector field\n",
    "# (so e.g. a zero output would be the identity warping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d14dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "warp = monai.networks.blocks.Warp()\n",
    "\n",
    "# Use example reg_net output to apply warp\n",
    "example_warped_image = warp(\n",
    "    data_item['img12'][[1],:,:,:].unsqueeze(0), # moving image\n",
    "    reg_net_example_output # warping\n",
    ")\n",
    "\n",
    "# Uncomment to preview warped image from forward pass example above\n",
    "# preview_image(example_warped_image[0,0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57b8c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview deformation as a vector field plot\n",
    "\n",
    "def plot_2D_vector_field(vector_field, downsampling):\n",
    "    \"\"\"Vector_field should be a tensor of shape (2,L,W).\n",
    "    The 0 dimension is the two components of the output vectors\"\"\"\n",
    "    downsample2D = monai.networks.layers.factories.Pool['AVG',2](kernel_size=downsampling)\n",
    "    vf_downsampled = downsample2D(vector_field.unsqueeze(0))[0]\n",
    "    plt.quiver(vf_downsampled[0,:,:], vf_downsampled[1,:,:]);\n",
    "    \n",
    "\n",
    "def preview_3D_vector_field(vector_field):\n",
    "    \"\"\"Vector_field should be a tensor of shape (3,L,W,H).\n",
    "    The 0 dimension is the three components of the output vectors\"\"\"\n",
    "    \n",
    "    x,y,z = np.array(vector_field.shape[1:])//2 # half-way slices\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.subplot(1,3,1); plt.axis('off')\n",
    "    plot_2D_vector_field(vector_field[[1,2],x,:,:], 4)\n",
    "    plt.subplot(1,3,2); plt.axis('off')\n",
    "    plot_2D_vector_field(vector_field[[0,2],:,y,:], 4)\n",
    "    plt.subplot(1,3,3); plt.axis('off')\n",
    "    plot_2D_vector_field(vector_field[[0,1],:,:,z], 4)\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to preview displacement field from example forward pass of reg_net above\n",
    "# preview_3D_vector_field(reg_net_example_output.detach()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b681022",
   "metadata": {},
   "source": [
    "## Image similarity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1f4f76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local normalized cross-correlation loss.\n",
    "# The paper does a global NCC, but MONAI provides local and it's easy to use.\n",
    "# Another difference from the paper: instead of returning 1-LNCC this will return -LNCC\n",
    "\n",
    "lncc_loss = monai.losses.LocalNormalizedCrossCorrelationLoss(\n",
    "    ndim=3, # this keyword argument will soon be renamed to spatial_dims\n",
    "    kernel_size=3,\n",
    "    kernel_type='rectangular',\n",
    "    reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5156db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try out the image similarity loss on the example.\n",
    "\n",
    "# lncc_loss(\n",
    "#     example_warped_image, # registered image\n",
    "#     data_item['img12'][[0],:,:,:].unsqueeze(0) # target (\"fixed image\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da58e16",
   "metadata": {},
   "source": [
    "## Regularization loss for registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f56423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bending_loss = monai.losses.BendingEnergyLoss()\n",
    "\n",
    "# Uncomment to try out the bending energy loss on the example\n",
    "# bending_loss(reg_net_example_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc030353",
   "metadata": {},
   "source": [
    "# Segmentation network pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cell, uncomment when you want to train\n",
    "\n",
    "# seg_net.to('cuda')\n",
    "\n",
    "# dataloader_seg_available_train = monai.data.DataLoader(\n",
    "#     dataset_seg_available_train,\n",
    "#     batch_size=1,\n",
    "#     num_workers=4,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# dataloader_seg_available_valid = monai.data.DataLoader(\n",
    "#     dataset_seg_available_valid,\n",
    "#     batch_size=1,\n",
    "#     num_workers=4,\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# learning_rate = 1e-4\n",
    "# optimizer = torch.optim.Adam(seg_net.parameters(), learning_rate)\n",
    "\n",
    "# max_epochs = 30\n",
    "# training_losses = []\n",
    "# validation_losses = []\n",
    "\n",
    "# for epoch_number in range(max_epochs):\n",
    "    \n",
    "    \n",
    "#     seg_net.train()\n",
    "#     losses = []\n",
    "#     for batch in dataloader_seg_available_train:\n",
    "#         imgs = batch['img'].to('cuda')\n",
    "#         true_segs = batch['seg'].to('cuda')\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         predicted_segs = seg_net(imgs)\n",
    "#         loss = dice_loss(predicted_segs, true_segs)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         losses.append(loss.item())\n",
    "    \n",
    "#     training_loss = np.mean(losses)\n",
    "    \n",
    "#     seg_net.eval()\n",
    "#     losses = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader_seg_available_valid:\n",
    "#             imgs = batch['img'].to('cuda')\n",
    "#             true_segs = batch['seg'].to('cuda')\n",
    "#             predicted_segs = seg_net(imgs)\n",
    "#             loss = dice_loss(predicted_segs, true_segs)\n",
    "#             losses.append(loss.item())\n",
    "#         validation_loss = np.mean(losses)\n",
    "        \n",
    "#     print(f\"training loss: {training_loss}, validation loss: {validation_loss}\")\n",
    "#     preview_image(torch.argmax(torch.softmax(seg_net(dataset_seg_available_valid[0]['img'].unsqueeze(0).cuda()),dim=1),dim=1, keepdim=True)[0,0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d031795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; SAVE\n",
    "# torch.save(seg_net.state_dict(),'seg_net_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffbfab83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECKPOINT CELL; LOAD\n",
    "seg_net.load_state_dict(torch.load('seg_net_pretrained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68ec1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get seg_net out of graphics memory to make room for other things coming up next\n",
    "seg_net.to('cpu');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91d2f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to try out pretrained seg net on one of the validation images\n",
    "\n",
    "# test_input = dataset_seg_available_valid[0]['img'].unsqueeze(0)\n",
    "# test_seg_predicted = seg_net(test_input)\n",
    "\n",
    "# # Original image from validation set\n",
    "# preview_image(dataset_seg_available_valid[0]['img'][0])\n",
    "# # Ground truth segmentation\n",
    "# preview_image(dataset_seg_available_valid[0]['seg'][0])\n",
    "# # Our predicted segmentation\n",
    "# seg_net.eval()\n",
    "# preview_image(torch.argmax(torch.softmax(seg_net(dataset_seg_available_valid[0]['img'].unsqueeze(0)),dim=1),dim=1, keepdim=True)[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620ef58f",
   "metadata": {},
   "source": [
    "# Training both in alternation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8af785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cell, uncomment when you want to train\n",
    "\n",
    "reg_net.to('cuda')\n",
    "\n",
    "dataloader_pairs_train = monai.data.DataLoader(\n",
    "    dataset_pairs_train,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_pairs_valid = monai.data.DataLoader(\n",
    "    dataset_pairs_valid,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "dataloader_pairs_train_seg_trainable = monai.data.DataLoader(\n",
    "    dataset_pairs_train_seg_trainable,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ") # TODO still need to use this; when seg-only part of alternation comes up\n",
    "\n",
    "learning_rate = 5e-4\n",
    "optimizer = torch.optim.Adam(seg_net.parameters(), learning_rate)\n",
    "\n",
    "max_epochs = 1\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch_number in range(max_epochs):\n",
    "    \n",
    "    reg_net.train()\n",
    "    losses = []\n",
    "    for batch in dataloader_pairs_train:\n",
    "        img12 = batch['img12'].to('cuda') \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        displacement_fields = reg_net(img12)\n",
    "        warped_img2 = warp(img12[:,[1],:,:,:], displacement_fields)  \n",
    "        loss_similarity = lncc_loss(\n",
    "            warped_img2, # prediciton\n",
    "            img12[:,[0],:,:,:] # target\n",
    "        )\n",
    "        loss_similarity.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\" \",loss_similarity.item())\n",
    "        losses.append(loss_similarity.item())\n",
    "    \n",
    "    training_loss = np.mean(losses)\n",
    "    \n",
    "    reg_net.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader_pairs_valid:\n",
    "            img12 = batch['img12'].to('cuda')\n",
    "            # img21 = img12[:,[1,0],:,:,:]\n",
    "            displacement_fields = reg_net(img12)\n",
    "            warped_img2 = warp(img12[:,[1],:,:,:], displacement_fields)\n",
    "            loss_similarity = lncc_loss(\n",
    "                warped_img2, # prediciton\n",
    "                img12[:,[0],:,:,:] # target\n",
    "            )\n",
    "            \n",
    "            losses.append(loss_similarity.item())\n",
    "        validation_loss = np.mean(losses)\n",
    "        \n",
    "    print(f\"training loss: {training_loss}, validation loss: {validation_loss}\")\n",
    "#     preview_image(torch.argmax(torch.softmax(seg_net(dataset_seg_available_valid[0]['img'].unsqueeze(0).cuda()),dim=1),dim=1, keepdim=True)[0,0].cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
